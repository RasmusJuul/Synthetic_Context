{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "871a9c5b-fdd2-4c67-8393-b57474338f0a",
   "metadata": {},
   "source": [
    "# Run CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aee0b08f-6a69-4f37-a6b1-ec4c2b66d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.data.dataloaders import  CycleGANDataModule\n",
    "from src.models.CycleGan import CycleGan\n",
    "import tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53af457-1638-4f20-8e8c-01c726dd0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toImage(x):\n",
    "    x = x.squeeze()\n",
    "    x = x.numpy()\n",
    "    x = x.astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f527851-cc1a-4774-a508-e0086ab72840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09379b5e-f545-4319-9272-cbf200af4034",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CycleGan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "351cd3d6-0121-4017-bc01-13bf8c072e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/CycleGAN_TPWN-2023-11-29-2038/CycleGAN-epoch=69.ckpt\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=None)['state_dict'], strict=True)\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "model = torch.compile(model)\n",
    "model = model.cuda()\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c63c2b7-e453-4385-96f9-43ba64596ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = CycleGANDataModule(batch_size=1, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73d24518-9a00-4474-9133-8c1ef18bb663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images of validationA and 100 images of validationB\n"
     ]
    }
   ],
   "source": [
    "datamodule.setup(stage=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "234c13a8-231a-4320-b9a6-e33e17738d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03c62e95-c148-4324-9c29-1e52ee9ef950",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f78e747c-5220-4ef7-95e3-6d0066ff9e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b72a80bc-3dca-470a-9489-2ccf35dd0790",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgA = batch[\"A\"]\n",
    "imgA = imgA.to(\"cuda\")\n",
    "imgB = batch[\"B\"]\n",
    "imgB = imgB.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec51110d-275d-4da4-9529-ce2135b8bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeB = torch.nn.functional.sigmoid(model.genX(imgA))*255 \n",
    "cycledA = torch.nn.functional.sigmoid(model.genY(fakeB))*255 \n",
    "\n",
    "fakeA = torch.nn.functional.sigmoid(model.genY(imgB))*255 \n",
    "cycledB = torch.nn.functional.sigmoid(model.genX(fakeA))*255 \n",
    "\n",
    "sameB = torch.nn.functional.sigmoid(model.genX(imgB))*255 \n",
    "sameA = torch.nn.functional.sigmoid(model.genY(imgA))*255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68cf84a4-8c6c-4be5-935a-7c0a8b6b27ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeA = toImage(fakeA.cpu().detach())\n",
    "fakeB = toImage(fakeB.cpu().detach())\n",
    "sameA = toImage(sameA.cpu().detach())\n",
    "sameB = toImage(sameB.cpu().detach())\n",
    "cycledA = toImage(cycledA.cpu().detach())\n",
    "cycledB = toImage(cycledB.cpu().detach())\n",
    "imgA = imgA.squeeze().to(torch.uint8).cpu().numpy()\n",
    "imgB = imgB.squeeze().to(torch.uint8).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdaef696-13f6-4e9a-9530-335484c61f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tifffile.imwrite(\"fakeA.tif\",fakeA)\n",
    "tifffile.imwrite(\"fakeB.tif\",fakeB)\n",
    "tifffile.imwrite(\"sameA.tif\",sameA)\n",
    "tifffile.imwrite(\"sameB.tif\",sameB)\n",
    "tifffile.imwrite(\"cycledA.tif\",cycledA)\n",
    "tifffile.imwrite(\"cycledB.tif\",cycledB)\n",
    "tifffile.imwrite(\"imgA.tif\",imgA)\n",
    "tifffile.imwrite(\"imgB.tif\",imgB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8410ac3c-e7a9-4e80-91a4-7dd805eb05ed",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d530393-5701-4fd0-a0e5-6f99347b0145",
   "metadata": {},
   "source": [
    "98.5% accuracy of metric on synthetic mixes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8e713b-7a99-4a56-b76b-42abc8c8937e",
   "metadata": {},
   "source": [
    "# Test model on metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708765f7-cc7e-449c-a83f-1cf91ee32efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import _PATH_DATA, _PATH_MODELS, _PROJECT_ROOT\n",
    "from src.data.dataloaders import MetricDataset#, BugNISTDataModule\n",
    "from src.models.unet import UNet_pl\n",
    "from src.models.swin_unetr import SwinUNETR_pl as SwinUNETR\n",
    "import torch\n",
    "import torch._dynamo\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import tifffile\n",
    "from skimage import measure\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d27c620d-e269-4b32-b491-5c44a23ea521",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "# model = UNet_pl(\n",
    "#     spatial_dims=3,\n",
    "#     in_channels=1,\n",
    "#     out_channels=13,\n",
    "#     channels=(4, 8, 16, 32, 64),\n",
    "#     strides=(2, 2, 2, 2),\n",
    "#     lr=1,\n",
    "# )\n",
    "# model = UNet_pl(\n",
    "#         spatial_dims=3,\n",
    "#         in_channels=1,\n",
    "#         out_channels=13,\n",
    "#         channels=(16, 32, 64, 128, 256, 512),\n",
    "#         strides=(2, 2, 2, 2, 2),\n",
    "#         num_res_units = 3,\n",
    "#     )\n",
    "model = SwinUNETR(img_size=(256,128,128), in_channels=1, out_channels=13, feature_size=24)\n",
    "\n",
    "# model_path = \"../models/small50000v3-2024-01-29-1606/UNet_small-epoch=79.ckpt\"\n",
    "# model_path = \"../models/large50000v3-2024-02-02-0054/UNet_large-epoch=36.ckpt\"\n",
    "model_path = \"../models/swin20000v3-2024-02-04-0838/SwinUNETR-epoch=5.ckpt\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=None)['state_dict'], strict=True)\n",
    "    \n",
    "torch._dynamo.config.suppress_errors = True\n",
    "# model = torch.compile(model)\n",
    "model.eval();\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "705bd20e-a2da-429e-ae4e-1b1dfb281a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(MetricDataset(),\n",
    "                         batch_size=1,\n",
    "                         num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c6019da-dd41-4857-b30f-97bf8d708648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 388/388 [27:15<00:00,  4.22s/batch]\n"
     ]
    }
   ],
   "source": [
    "color_dict = {1:[255,0,0],2:[0,255,0],3:[0,0,255],4:[255,255,0],5:[255,0,255],6:[0,255,255],7:[161,161,255],8:[171,128,84],9:[255,128,191],10:[135,89,179],11:[255,191,128],12:[0,85,0]}\n",
    "accuracies = []\n",
    "for k,(img,label) in enumerate(tqdm(test_loader, unit=\"batch\")):\n",
    "    img = img.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        preds = model(img);\n",
    "        preds_sm = preds.softmax(dim=1).cpu().numpy();\n",
    "\n",
    "        class_props = []\n",
    "        for i in range(1, 13):  # For each class channel\n",
    "            props = measure.regionprops(\n",
    "        \n",
    "                #                        v-- softmax channel for this class\n",
    "        \n",
    "                measure.label(preds_sm[0, i] > 0.25)  # Threshold softmax probability at 0.25\n",
    "        \n",
    "            )\n",
    "        \n",
    "            props = [p for p in props if p.area > 5**3]  # Remove small connected components\n",
    "        \n",
    "            class_props.append(props)\n",
    "        \n",
    "        for i, props in enumerate(class_props):\n",
    "            # For every found connected component\n",
    "            for p in props:\n",
    "                bb = p.bbox\n",
    "        \n",
    "                # Sets the found connected component to the mean value of the connected component\n",
    "                preds_sm[0,i+1,bb[0]:bb[3],bb[1]:bb[4],bb[2]:bb[5]][p.image] = preds_sm[0,i+1,bb[0]:bb[3],bb[1]:bb[4],bb[2]:bb[5]][p.image].mean()\n",
    "        \n",
    "        out = torch.Tensor(preds_sm).softmax(dim=1).argmax(dim=1).to(torch.uint8)\n",
    "        accuracies.append(sum(out[out != 0] == label[out != 0])/len(label[out != 0]))\n",
    "        \n",
    "        comparison = torch.zeros(label.shape)\n",
    "        \n",
    "        comparison[out == label] = 2 # Finds correctly labelled pixels\n",
    "        comparison[out != label] = 1 # Finds wrongly labelled pixels\n",
    "        comparison[out == 0] = 0    # Removes background pixels\n",
    "        \n",
    "        \n",
    "        rgba_img = torch.zeros(256,128,128,4)\n",
    "        \n",
    "        rgba_img[:,:,:,1][comparison[0] == 2] = 128 # Set colour of correctly labelled pixels to green\n",
    "        rgba_img[:,:,:,3][comparison[0] == 2] = 128 # Set transparency of correctly labelled pixels\n",
    "        rgba_img[:,:,:,0][comparison[0] == 1] = 128 # Set colour of wrongly labelled pixels to red\n",
    "        rgba_img[:,:,:,3][comparison[0] == 1] = 128 # Set transparency of wrongly labelled pixels\n",
    "        rgba_img[:,:,:,3][comparison[0] == 0] = 255 # Set transparency of background (fully transparent)\n",
    "        tifffile.imwrite(f\"results_swin/{test_loader.dataset.get_name_of_image(k)}_acc={accuracies[k]:.3f}.tif\",rgba_img.to(torch.uint8).numpy())\n",
    "\n",
    "        rgba_img = torch.zeros(256,128,128,4)\n",
    "        for i in range(0,13):\n",
    "            if i == 0:\n",
    "                rgba_img[:,:,:,3][(out == i).squeeze()] = 255\n",
    "            else:\n",
    "                rgba_img[:,:,:,0][(out == i).squeeze()] = color_dict[i][0]\n",
    "                rgba_img[:,:,:,1][(out == i).squeeze()] = color_dict[i][1]\n",
    "                rgba_img[:,:,:,2][(out == i).squeeze()] = color_dict[i][2]\n",
    "                rgba_img[:,:,:,3][(out == i).squeeze()] = 128\n",
    "        tifffile.imwrite(f\"results_swin/{test_loader.dataset.get_name_of_image(k)}_acc={accuracies[k]:.3f}_coloured.tif\",rgba_img.to(torch.uint8).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62892641-aeef-480f-8bc5-3cab7a631e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = torch.Tensor(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b053e410-43a7-4543-bb88-3a0e85f884d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1733)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "073e7f87-1a89-4805-80c3-46f9b866aba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(335)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dc86757-2603-4701-9254-6bed12f40ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9979)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72210951-d682-497a-b542-171bf2251bc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work3/s164564/Thesis/Synthetic_Context/data/cyclegan_256/trainB/02/Mix 6_007.tif'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader.dataset.image_paths[257]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356889db-d182-4c84-8ac5-3a23181a5a13",
   "metadata": {},
   "source": [
    "19.7% accuracy with model train on synthetic mixes\n",
    "\n",
    "20.2% accuracy with model train on GAN images\n",
    "\n",
    "26.7% accuracy with model train on new and improved synthetic mixes\n",
    "\n",
    "29.0% accuracy with model train on new and improved synthetic mixes with background noise\n",
    "\n",
    "30.5% accuracy with connected component analysis on softmax predictions (and improved synthetic mixes)\n",
    "\n",
    "51.4% accuracy with large model (and all previous improvements) (5000 data points)\n",
    "\n",
    "59.3%/56.1%/63.2%/62.1% accuracy with large model (and all previous improvements) (20000 data points)\n",
    "\n",
    "57.1%/58.6%/57.7%/56.9% accuracy with large model and umap subset (14368 data points)\n",
    "\n",
    "59.3%/59.3%/57.5% accuracy with large model and pca subset (16742 data points)\n",
    "\n",
    "63.8% accuracy with large model (and all previous improvements) (~50000 data points)\n",
    "\n",
    "61.4% accuracy with large model and feature distance subset (20000 data points) without skip-connections\n",
    "\n",
    "62.2% accuracy SwinUNETR (~50000 data points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6381f80-61ef-4ff6-81dd-f67257fe078d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc887114-bf2a-4bd8-89aa-97e5c7e499f9",
   "metadata": {},
   "source": [
    "# Testing old models on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7679445-dfd1-44d8-94af-06106a305404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import _PATH_DATA, _PATH_MODELS, _PROJECT_ROOT\n",
    "from src.data.dataloaders import BugNISTDataModule\n",
    "from src.models.unet import UNet_pl\n",
    "import torch\n",
    "import torch._dynamo\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import tifffile\n",
    "from skimage import measure\n",
    "import numpy as np\n",
    "from torchmetrics import Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d74a647-de76-4220-8f43-dd6630ac6485",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "# model = UNet_pl(\n",
    "#     spatial_dims=3,\n",
    "#     in_channels=1,\n",
    "#     out_channels=13,\n",
    "#     channels=(4, 8, 16, 32, 64),\n",
    "#     strides=(2, 2, 2, 2),\n",
    "#     lr=1,\n",
    "# )\n",
    "model = UNet_pl(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=13,\n",
    "        channels=(16, 32, 64, 128, 256, 512),\n",
    "        strides=(2, 2, 2, 2, 2),\n",
    "        num_res_units = 3,\n",
    "    )\n",
    "\n",
    "model_path = \"../models/UNet_large-2023-12-06-1549/UNet-epoch=79.ckpt\"\n",
    "# model_path = \"../models/UNet_no_noise-2023-11-13-1713/UNet-epoch=343.ckpt\"\n",
    "# model_path = \"../models/UNet_old-2023-09-20-2208/UNet-epoch=229.ckpt\"\n",
    "# model_path = \"../models/UNet-2023-11-03-0950/UNet-epoch=498.ckpt\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=None)['state_dict'], strict=True)\n",
    "    \n",
    "torch._dynamo.config.suppress_errors = True\n",
    "model = torch.compile(model)\n",
    "model.eval();\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70017cb-bbfd-48ba-b5c3-4f7703d01b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugnist = BugNISTDataModule(batch_size=10, num_workers=0, mix=True,no_noise=False,old=False)\n",
    "bugnist.setup()\n",
    "\n",
    "test_loader = bugnist.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d33c836e-a4f5-4a83-a3ab-11abfd95dc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 50/50 [46:54<00:00, 56.29s/batch]\n"
     ]
    }
   ],
   "source": [
    "color_dict = {1:[255,0,0],2:[0,255,0],3:[0,0,255],4:[255,255,0],5:[255,0,255],6:[0,255,255],7:[161,161,255],8:[171,128,84],9:[255,128,191],10:[135,89,179],11:[255,191,128],12:[0,85,0]}\n",
    "accuracies = []\n",
    "dice_scores = []\n",
    "dice_score = Dice(num_classes=13, ignore_index=0)\n",
    "for k,(img,label) in enumerate(tqdm(test_loader, unit=\"batch\")):\n",
    "    img = img.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        preds = model(img);\n",
    "        preds_sm = preds.softmax(dim=1).cpu().numpy();\n",
    "\n",
    "        class_props = []\n",
    "        for i in range(1, 13):  # For each class channel\n",
    "            props = measure.regionprops(\n",
    "        \n",
    "                #                        v-- softmax channel for this class\n",
    "        \n",
    "                measure.label(preds_sm[0, i] > 0.25)  # Threshold softmax probability at 0.5\n",
    "        \n",
    "            )\n",
    "        \n",
    "            props = [p for p in props if p.area > 5**3]  # Remove small connected components\n",
    "        \n",
    "            class_props.append(props)\n",
    "        \n",
    "        for i, props in enumerate(class_props):\n",
    "            # For every found connected component\n",
    "            for p in props:\n",
    "                bb = p.bbox\n",
    "        \n",
    "                # Sets the found connected component to the mean value of the connected component\n",
    "                preds_sm[0,i+1,bb[0]:bb[3],bb[1]:bb[4],bb[2]:bb[5]][p.image] = preds_sm[0,i+1,bb[0]:bb[3],bb[1]:bb[4],bb[2]:bb[5]][p.image].mean()\n",
    "        \n",
    "        out = torch.Tensor(preds_sm).softmax(dim=1).argmax(dim=1).to(torch.uint8)\n",
    "        dice_scores.append(dice_score(out,label))\n",
    "        accuracies.append(sum(out[out != 0] == label[out != 0])/len(label[out != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59620cdb-5702-47c5-b213-b96e87dec5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dice score tensor(0.9117)\n",
      "accuracy tensor(0.9047)\n"
     ]
    }
   ],
   "source": [
    "print(\"dice score\",torch.Tensor(dice_scores).mean())\n",
    "print(\"accuracy\",torch.Tensor(accuracies).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8cb52b-08dc-4dca-ab52-f3303f4902f3",
   "metadata": {},
   "source": [
    "|                       Model                       | Dice score | Accuracy |\r\n",
    "|:-------------------------------------------------:|:----------:|:--------:|\r\n",
    "|                   Loose Packing                   |       0.41 |     0.40 |\r\n",
    "|                 Tight Packing (nn)                |       0.45 |     0.43 |\r\n",
    "|                 Tight Packing (wn)                |       0.65 |     0.64 |\r\n",
    "| Majority voting of 3 models<br>Tight Packing (wn) |       0.69 |     0.69 |.69 \n",
    "|                  Large model TPWN                 |       0.91 |     0.90 |   ||0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb102ee-7c2d-4a07-88b4-0aa540e55b2d",
   "metadata": {},
   "source": [
    "                                                                        nn = no noise     wn = with noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f39d991-7d75-4d38-8976-8702da54e60c",
   "metadata": {},
   "source": [
    "# Testing majority voting of multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f4355e6-22d5-466c-a6a0-56179273fa96",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from src import _PATH_DATA, _PATH_MODELS, _PROJECT_ROOT\n",
    "from src.data.dataloaders import BugNISTDataModule, MetricDataset\n",
    "from src.models.unet import UNet_pl\n",
    "import torch\n",
    "import torch._dynamo\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import tifffile\n",
    "from skimage import measure\n",
    "import numpy as np\n",
    "from torchmetrics import Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddadf76f-ffa5-4568-8d77-13719f357dfc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# bugnist = BugNISTDataModule(batch_size=32, num_workers=0, mix=True,no_noise=False,old=False)\n",
    "# bugnist.setup()\n",
    "\n",
    "# test_loader = bugnist.test_dataloader()\n",
    "\n",
    "\n",
    "test_loader = DataLoader(MetricDataset(),\n",
    "                         batch_size=1,\n",
    "                         num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f26a838-cfae-4234-b27e-d194b7059938",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "model_1 = UNet_pl(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=13,\n",
    "        channels=(16, 32, 64, 128, 256, 512),\n",
    "        strides=(2, 2, 2, 2, 2),\n",
    "        num_res_units = 3,\n",
    "    )\n",
    "model_2 = UNet_pl(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=13,\n",
    "        channels=(16, 32, 64, 128, 256, 512),\n",
    "        strides=(2, 2, 2, 2, 2),\n",
    "        num_res_units = 3,\n",
    "    )\n",
    "model_3 = UNet_pl(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=13,\n",
    "        channels=(16, 32, 64, 128, 256, 512),\n",
    "        strides=(2, 2, 2, 2, 2),\n",
    "        num_res_units = 3,\n",
    "    )\n",
    "model_4 = UNet_pl(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=13,\n",
    "        channels=(16, 32, 64, 128, 256, 512),\n",
    "        strides=(2, 2, 2, 2, 2),\n",
    "        num_res_units = 3,\n",
    "    )\n",
    "model_5 = UNet_pl(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=13,\n",
    "        channels=(16, 32, 64, 128, 256, 512),\n",
    "        strides=(2, 2, 2, 2, 2),\n",
    "        num_res_units = 3,\n",
    "    )\n",
    "\n",
    "\n",
    "model_1_path = f\"{_PATH_MODELS}/UNet_large-2023-12-15-1448/UNet-epoch=38.ckpt\"\n",
    "model_2_path = f\"{_PATH_MODELS}/UNet_large2-2024-01-03-1225/UNet-epoch=44.ckpt\"\n",
    "model_3_path = f\"{_PATH_MODELS}/UNet_large3-2024-01-04-1338/UNet-epoch=58.ckpt\"\n",
    "model_4_path = f\"{_PATH_MODELS}/UNet_large4-2024-01-04-1905/UNet-epoch=66.ckpt\"\n",
    "model_5_path = f\"{_PATH_MODELS}/UNet_large5-2024-01-05-2309/UNet-epoch=40.ckpt\"\n",
    "\n",
    "model_1.load_state_dict(torch.load(model_1_path, map_location=None)['state_dict'], strict=True)\n",
    "model_2.load_state_dict(torch.load(model_2_path, map_location=None)['state_dict'], strict=True)\n",
    "model_3.load_state_dict(torch.load(model_3_path, map_location=None)['state_dict'], strict=True)\n",
    "model_4.load_state_dict(torch.load(model_4_path, map_location=None)['state_dict'], strict=True)\n",
    "model_5.load_state_dict(torch.load(model_5_path, map_location=None)['state_dict'], strict=True)\n",
    "    \n",
    "torch._dynamo.config.suppress_errors = True\n",
    "model_1 = torch.compile(model_1)\n",
    "model_2 = torch.compile(model_2)\n",
    "model_3 = torch.compile(model_3)\n",
    "model_4 = torch.compile(model_4)\n",
    "model_5 = torch.compile(model_5)\n",
    "model_1.eval();\n",
    "model_2.eval();\n",
    "model_3.eval();\n",
    "model_4.eval();\n",
    "model_5.eval();\n",
    "model_1 = model_1.to(\"cuda\")\n",
    "model_2 = model_2.to(\"cuda\")\n",
    "model_3 = model_3.to(\"cuda\")\n",
    "model_4 = model_3.to(\"cuda\")\n",
    "model_5 = model_3.to(\"cuda\")\n",
    "\n",
    "models = [model_1,model_2,model_3,model_4,model_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d93026-5871-452d-afcd-2ebc459289b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 388/388 [1:09:28<00:00, 10.74s/batch]\n"
     ]
    }
   ],
   "source": [
    "color_dict = {1:[255,0,0],2:[0,255,0],3:[0,0,255],4:[255,255,0],5:[255,0,255],6:[0,255,255],7:[161,161,255],8:[171,128,84],9:[255,128,191],10:[135,89,179],11:[255,191,128],12:[0,85,0]}\n",
    "accuracies = []\n",
    "# dice_scores = []\n",
    "# dice_score = Dice(num_classes=13, ignore_index=0)\n",
    "for k,(img,label) in enumerate(tqdm(test_loader, unit=\"batch\")):\n",
    "    img = img.to(\"cuda\")\n",
    "    outs = []\n",
    "    with torch.no_grad():\n",
    "        for model in models:\n",
    "            preds = model(img);\n",
    "            preds_sm = preds.softmax(dim=1).cpu().numpy();\n",
    "    \n",
    "            class_props = []\n",
    "            for i in range(1, 13):  # For each class channel\n",
    "                props = measure.regionprops(\n",
    "            \n",
    "                    #                        v-- softmax channel for this class\n",
    "            \n",
    "                    measure.label(preds_sm[0, i] > 0.25)  # Threshold softmax probability at 0.5\n",
    "            \n",
    "                )\n",
    "            \n",
    "                props = [p for p in props if p.area > 5**3]  # Remove small connected components\n",
    "            \n",
    "                class_props.append(props)\n",
    "            \n",
    "            for i, props in enumerate(class_props):\n",
    "                # For every found connected component\n",
    "                for p in props:\n",
    "                    bb = p.bbox\n",
    "            \n",
    "                    # Sets the found connected component to the mean value of the connected component\n",
    "                    preds_sm[0,i+1,bb[0]:bb[3],bb[1]:bb[4],bb[2]:bb[5]][p.image] = preds_sm[0,i+1,bb[0]:bb[3],bb[1]:bb[4],bb[2]:bb[5]][p.image].mean()\n",
    "            \n",
    "            out = torch.Tensor(preds_sm).softmax(dim=1).argmax(dim=1).to(torch.uint8)\n",
    "            outs.append(out)\n",
    "        outs = [out.unsqueeze(dim=0) for out in outs]\n",
    "        out = torch.concat(outs,dim=0).mode(0).values\n",
    "        # dice_scores.append(dice_score(out,label))\n",
    "        accuracies.append(sum(out[out != 0] == label[out != 0])/len(label[out != 0]))\n",
    "\n",
    "        comparison = torch.zeros(label.shape)\n",
    "        # temp = pred.softmax(dim=1).argmax(dim=1).to(torch.uint8).cpu()\n",
    "        \n",
    "        comparison[out == label] = 2 # Finds correctly labelled pixels\n",
    "        comparison[out != label] = 1 # Finds wrongly labelled pixels\n",
    "        comparison[out == 0] = 0    # Removes background pixels\n",
    "        \n",
    "        \n",
    "        rgba_img = torch.zeros(256,128,128,4)\n",
    "        \n",
    "        rgba_img[:,:,:,1][comparison[0] == 2] = 128 # Set colour of correctly labelled pixels to green\n",
    "        rgba_img[:,:,:,3][comparison[0] == 2] = 128 # Set transparency of correctly labelled pixels\n",
    "        rgba_img[:,:,:,0][comparison[0] == 1] = 128 # Set colour of wrongly labelled pixels to red\n",
    "        rgba_img[:,:,:,3][comparison[0] == 1] = 128 # Set transparency of wrongly labelled pixels\n",
    "        rgba_img[:,:,:,3][comparison[0] == 0] = 255 # Set transparency of background (fully transparent)\n",
    "        tifffile.imwrite(f\"outputs_combined/{test_loader.dataset.get_name_of_image(k)}_acc={accuracies[k]:.3f}.tif\",rgba_img.to(torch.uint8).numpy())\n",
    "\n",
    "        rgba_img = torch.zeros(256,128,128,4)\n",
    "        for i in range(0,13):\n",
    "            if i == 0:\n",
    "                rgba_img[:,:,:,3][(out == i).squeeze()] = 255\n",
    "            else:\n",
    "                rgba_img[:,:,:,0][(out == i).squeeze()] = color_dict[i][0]\n",
    "                rgba_img[:,:,:,1][(out == i).squeeze()] = color_dict[i][1]\n",
    "                rgba_img[:,:,:,2][(out == i).squeeze()] = color_dict[i][2]\n",
    "                rgba_img[:,:,:,3][(out == i).squeeze()] = 128\n",
    "        tifffile.imwrite(f\"outputs_combined/{test_loader.dataset.get_name_of_image(k)}_acc={accuracies[k]:.3f}_coloured.tif\",rgba_img.to(torch.uint8).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "163ab6bb-2c2c-4919-8425-5d1df8c3c09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy tensor(0.6319)\n"
     ]
    }
   ],
   "source": [
    "# print(\"dice score\",torch.Tensor(dice_scores).mean())\n",
    "print(\"accuracy\",torch.Tensor(accuracies).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f1bb2c-2b21-4c95-9de3-c07c1ce3ab59",
   "metadata": {},
   "source": [
    "# Distance between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "014a0cf9-4405-4eeb-a31c-29a415da900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src import _PATH_DATA, _PATH_MODELS, _PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d25c7a11-0e06-4f41-9c58-9f3474224bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "real = np.load(\"features/real_UNetNoSkipConnection_features.npy\")\n",
    "# test = np.load(\"features/tight_packing_wn_test_UNetNoSkipConnection_features.npy\")\n",
    "train = np.load(\"features/tight_packing_wn_train_UNetNoSkipConnection_features.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d20546a4-5adf-476c-b8d2-420307af97aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_distance = cdist(real,real)\n",
    "real_distance = np.ma.array(real_distance, mask=False)\n",
    "for i in range(real_distance.shape[0]):\n",
    "    real_distance.mask[i,i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39726a7f-00c4-412d-a99a-c5f99526471f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "857.914875608836"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_distance.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69745536-97d5-41be-a5f4-4ce71d05ae77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.05536610115451"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_distance.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f897aebe-bcbc-4cf3-a779-81cc589fa571",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distance = cdist(real,train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b2de424-2820-4a38-a6fa-4d4c22dea610",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distance_mean = train_distance.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57a7e657-bd91-4388-a2fd-5ee5d3a49a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_distance_mean,columns=[\"distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78decf60-3e69-4281-b47d-3a87094c2a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24944</th>\n",
       "      <td>808.647878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47697</th>\n",
       "      <td>809.346369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6212</th>\n",
       "      <td>809.863321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20322</th>\n",
       "      <td>811.495008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47149</th>\n",
       "      <td>811.531382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37831</th>\n",
       "      <td>1008.225646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8028</th>\n",
       "      <td>1008.260992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7706</th>\n",
       "      <td>1015.085494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>1021.480253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6741</th>\n",
       "      <td>1030.954701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49963 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          distance\n",
       "24944   808.647878\n",
       "47697   809.346369\n",
       "6212    809.863321\n",
       "20322   811.495008\n",
       "47149   811.531382\n",
       "...            ...\n",
       "37831  1008.225646\n",
       "8028   1008.260992\n",
       "7706   1015.085494\n",
       "626    1021.480253\n",
       "6741   1030.954701\n",
       "\n",
       "[49963 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78c8d3db-8f51-4bc6-90df-e3e583c94041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24944</th>\n",
       "      <td>808.647878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47697</th>\n",
       "      <td>809.346369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6212</th>\n",
       "      <td>809.863321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20322</th>\n",
       "      <td>811.495008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47149</th>\n",
       "      <td>811.531382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44706</th>\n",
       "      <td>892.957909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4678</th>\n",
       "      <td>892.958018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9950</th>\n",
       "      <td>892.960552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28927</th>\n",
       "      <td>892.963581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7691</th>\n",
       "      <td>892.965892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         distance\n",
       "24944  808.647878\n",
       "47697  809.346369\n",
       "6212   809.863321\n",
       "20322  811.495008\n",
       "47149  811.531382\n",
       "...           ...\n",
       "44706  892.957909\n",
       "4678   892.958018\n",
       "9950   892.960552\n",
       "28927  892.963581\n",
       "7691   892.965892\n",
       "\n",
       "[20000 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=\"distance\")[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e86d8bf-27ec-4b36-ab41-3b781e9698a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"../data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "482eb1db-840a-4d64-b7be-20f2314e37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_distance_subset = pd.merge(data_train, df.sort_values(by=\"distance\")[:20000], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be23eaba-a46e-4157-821e-23536ff7c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_distance_subset.to_csv(\"train_feature_distance_subset.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57ccd3f8-02b0-4ad1-8869-b8b407c03617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "871.8911518503338"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_distance_subset.distance.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce37f0f9-17bb-4624-a846-1a5cd080e786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.785389666765184"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_distance_subset.distance.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "523d5931-ea25-4b36-8a7d-e0dd8c8279b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_dict = {\"BF\":0,\"CF\":0,\"PP\":0,\"MA\":0,\"BL\":0,\"ML\":0,\"SL\":0,\"WO\":0,\"BC\":0,\"GH\":0,\"AC\":0,\"GP\":0}\n",
    "total = 0\n",
    "for centroid in feature_distance_subset.centroid_path:\n",
    "    temp = pd.read_csv(f\"{_PATH_DATA}/{centroid}\",sep=\";\")\n",
    "    for c in temp.Caption:\n",
    "        dist_dict[c] += 1\n",
    "    total += len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6861bb94-ad90-4bf9-a735-368f6ff712ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BF': 9091,\n",
       " 'CF': 11800,\n",
       " 'PP': 12202,\n",
       " 'MA': 11987,\n",
       " 'BL': 12541,\n",
       " 'ML': 8511,\n",
       " 'SL': 7493,\n",
       " 'WO': 11808,\n",
       " 'BC': 9744,\n",
       " 'GH': 8226,\n",
       " 'AC': 9090,\n",
       " 'GP': 12296}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ce8205e1-1a03-40a8-ab44-d261dbe34299",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_dict = {\"BF\":0,\"CF\":0,\"PP\":0,\"MA\":0,\"BL\":0,\"ML\":0,\"SL\":0,\"WO\":0,\"BC\":0,\"GH\":0,\"AC\":0,\"GP\":0}\n",
    "total = 0\n",
    "for centroid in data_train.centroid_path:\n",
    "    temp = pd.read_csv(f\"{_PATH_DATA}/{centroid}\",sep=\";\")\n",
    "    for c in temp.Caption:\n",
    "        dist_dict[c] += 1\n",
    "    total += len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4df5531c-9926-49b5-8b37-bc4cf315f902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BF': 22777,\n",
       " 'CF': 29338,\n",
       " 'PP': 30486,\n",
       " 'MA': 29959,\n",
       " 'BL': 31169,\n",
       " 'ML': 21175,\n",
       " 'SL': 18903,\n",
       " 'WO': 29517,\n",
       " 'BC': 24524,\n",
       " 'GH': 20401,\n",
       " 'AC': 22654,\n",
       " 'GP': 30663}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fd89b9fb-71c9-4352-a5a6-8db1e0cac961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BF 0.07310489591290449\n",
      "CF 0.09416303447744619\n",
      "PP 0.09784764704749556\n",
      "MA 0.09615619162553038\n",
      "BL 0.10003979895110507\n",
      "ML 0.06796312819755686\n",
      "SL 0.06067093328540341\n",
      "WO 0.09473755159420476\n",
      "BC 0.07871205458875487\n",
      "GH 0.06547890334632149\n",
      "AC 0.07271011599468491\n",
      "GP 0.09841574497859201\n"
     ]
    }
   ],
   "source": [
    "for item in dist_dict.items():\n",
    "    print(item[0],item[1]/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2aa4b5-f695-4f96-a515-ffa199510eef",
   "metadata": {},
   "source": [
    "# Check accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e38679-6e03-47dd-a7b6-8fae391c85c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aef5632f-e548-4d1b-b5da-aa0b26ac4946",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(f\"results_small/*_acc=*_coloured.tif\")\n",
    "acc_dict = {}\n",
    "for file in files:\n",
    "    temp = file.split(\"/\")[1].split(\"_\")\n",
    "    name = \"_\".join([temp[0],temp[1]])\n",
    "    acc = temp[2].split(\"=\")[1]\n",
    "    acc_dict[name] = acc\n",
    "df_1 = pd.DataFrame.from_dict(acc_dict,orient=\"index\",columns=[\"Accuracy_Small\"])\n",
    "df_1.Accuracy_Small = df_1.Accuracy_Small.astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa8c17eb-50ac-4576-b0ed-f11c81748037",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(f\"results_large/*_acc=*_coloured.tif\")\n",
    "acc_dict = {}\n",
    "for file in files:\n",
    "    temp = file.split(\"/\")[1].split(\"_\")\n",
    "    name = \"_\".join([temp[0],temp[1]])\n",
    "    acc = temp[2].split(\"=\")[1]\n",
    "    acc_dict[name] = acc\n",
    "df_2 = pd.DataFrame.from_dict(acc_dict,orient=\"index\",columns=[\"Accuracy_Large\"])\n",
    "df_2.Accuracy_Large = df_2.Accuracy_Large.astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24b70144-8eee-4720-a710-9f24e2a62013",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(f\"results_swin/*_acc=*_coloured.tif\")\n",
    "acc_dict = {}\n",
    "for file in files:\n",
    "    temp = file.split(\"/\")[1].split(\"_\")\n",
    "    name = \"_\".join([temp[0],temp[1]])\n",
    "    acc = temp[2].split(\"=\")[1]\n",
    "    acc_dict[name] = acc\n",
    "df_3 = pd.DataFrame.from_dict(acc_dict,orient=\"index\",columns=[\"Accuracy_Swin\"])\n",
    "df_3.Accuracy_Swin = df_3.Accuracy_Swin.astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d86623d-e85f-4b0a-9ee6-e5030c8c5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = pd.merge(df_1,df_2, left_index=True, right_index=True)\n",
    "df_acc = pd.merge(df_acc,df_3, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01809d49-2b7f-4c50-9d77-a47b24b6f626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy_Small</th>\n",
       "      <th>Accuracy_Large</th>\n",
       "      <th>Accuracy_Swin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mix 20_011</th>\n",
       "      <td>0.067</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 11_009</th>\n",
       "      <td>0.075</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 3_010</th>\n",
       "      <td>0.122</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 8_002</th>\n",
       "      <td>0.140</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 30_004</th>\n",
       "      <td>0.142</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 9_007</th>\n",
       "      <td>0.870</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 11_013</th>\n",
       "      <td>0.882</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 9_003</th>\n",
       "      <td>0.886</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 8_000</th>\n",
       "      <td>0.918</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 9_013</th>\n",
       "      <td>0.968</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>388 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Accuracy_Small  Accuracy_Large  Accuracy_Swin\n",
       "Mix 20_011           0.067           0.400          0.338\n",
       "Mix 11_009           0.075           0.571          0.392\n",
       "Mix 3_010            0.122           0.663          0.519\n",
       "Mix 8_002            0.140           0.750          0.696\n",
       "Mix 30_004           0.142           0.320          0.460\n",
       "...                    ...             ...            ...\n",
       "Mix 9_007            0.870           0.695          0.874\n",
       "Mix 11_013           0.882           0.999          0.977\n",
       "Mix 9_003            0.886           1.000          0.930\n",
       "Mix 8_000            0.918           0.949          0.904\n",
       "Mix 9_013            0.968           0.913          0.705\n",
       "\n",
       "[388 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acc.sort_values(by=\"Accuracy_Small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e465b92-64fe-409a-b8e6-3df5e6bb0c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy_Small</th>\n",
       "      <th>Accuracy_Large</th>\n",
       "      <th>Accuracy_Swin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mix 22_004</th>\n",
       "      <td>0.241</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 8_004</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 13_003</th>\n",
       "      <td>0.680</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 4_006</th>\n",
       "      <td>0.292</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 13_004</th>\n",
       "      <td>0.485</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Accuracy_Small  Accuracy_Large  Accuracy_Swin\n",
       "Mix 22_004           0.241           0.665          0.896\n",
       "Mix 8_004            0.487           0.922          0.889\n",
       "Mix 13_003           0.680           0.731          0.447\n",
       "Mix 4_006            0.292           0.643          0.511\n",
       "Mix 13_004           0.485           0.611          0.612"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acc.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba044b2f-e889-4010-a53f-a3fd83081875",
   "metadata": {},
   "source": [
    "# Combine UNet and SwinUNETR correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb600801-4bdc-47ee-8585-db866376fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import _PATH_DATA, _PATH_MODELS, _PROJECT_ROOT\n",
    "from src.data.dataloaders import MetricDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import scipy.ndimage as ndi\n",
    "from src.data.dataloaders import Label\n",
    "\n",
    "color_dict = {1:[255,0,0],2:[0,255,0],3:[0,0,255],4:[255,255,0],5:[255,0,255],6:[0,255,255],7:[161,161,255],8:[171,128,84],9:[255,128,191],10:[135,89,179],11:[255,191,128],12:[0,85,0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66db9f83-e5d9-477f-899b-5fc73e390558",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(MetricDataset(),\n",
    "                         batch_size=1,\n",
    "                         num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3984e936-744b-4255-a4db-8fd0e140f094",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 388/388 [11:08<00:00,  1.72s/image]\n"
     ]
    }
   ],
   "source": [
    "for i, (img,_) in enumerate(tqdm(test_loader, unit=\"image\")):\n",
    "    new_labels = np.zeros_like(img)\n",
    "    filename = test_loader.dataset.get_name_of_image(i)\n",
    "    \n",
    "    files = glob(f\"outputs_SwinUNETR/{filename}_acc=*.tif\")\n",
    "    for file in files:\n",
    "        if \"coloured\" in file:\n",
    "            labels = tifffile.imread(file)\n",
    "        else:\n",
    "            correct = tifffile.imread(file)\n",
    "    mask = correct[:,:,:,1] != 0\n",
    "    \n",
    "    temp = np.zeros((labels[mask].shape[0]))\n",
    "    for item in color_dict.items():\n",
    "        temp_mask = np.all(labels[mask][:,:3] == item[1],axis=1)\n",
    "        temp[temp_mask] = item[0]\n",
    "    new_labels[:,:,mask] = temp\n",
    "\n",
    "    files = glob(f\"outputs_SwinUNETR2/{filename}_acc=*.tif\")\n",
    "    for file in files:\n",
    "        if \"coloured\" in file:\n",
    "            labels = tifffile.imread(file)\n",
    "        else:\n",
    "            correct = tifffile.imread(file)\n",
    "    mask = correct[:,:,:,1] != 0\n",
    "    \n",
    "    temp = np.zeros((labels[mask].shape[0]))\n",
    "    for item in color_dict.items():\n",
    "        temp_mask = np.all(labels[mask][:,:3] == item[1],axis=1)\n",
    "        temp[temp_mask] = item[0]\n",
    "    new_labels[:,:,mask] = temp\n",
    "\n",
    "    files = glob(f\"outputs_big_data/{filename}_acc=*.tif\")\n",
    "    for file in files:\n",
    "        if \"coloured\" in file:\n",
    "            labels = tifffile.imread(file)\n",
    "        else:\n",
    "            correct = tifffile.imread(file)\n",
    "    mask = correct[:,:,:,1] != 0\n",
    "    \n",
    "    temp = np.zeros((labels[mask].shape[0]))\n",
    "    for item in color_dict.items():\n",
    "        temp_mask = np.all(labels[mask][:,:3] == item[1],axis=1)\n",
    "        temp[temp_mask] = item[0]\n",
    "    new_labels[:,:,mask] = temp\n",
    "    tifffile.imwrite(f\"new_labels/{filename}_label.tif\",new_labels.astype(\"uint8\"))\n",
    "\n",
    "    rgba_img = torch.zeros(256,128,128,4)\n",
    "    for i in range(0,13):\n",
    "        if i == 0:\n",
    "            rgba_img[:,:,:,3][(new_labels == i).squeeze()] = 255\n",
    "        else:\n",
    "            rgba_img[:,:,:,0][(new_labels == i).squeeze()] = color_dict[i][0]\n",
    "            rgba_img[:,:,:,1][(new_labels == i).squeeze()] = color_dict[i][1]\n",
    "            rgba_img[:,:,:,2][(new_labels == i).squeeze()] = color_dict[i][2]\n",
    "            rgba_img[:,:,:,3][(new_labels == i).squeeze()] = 128\n",
    "    tifffile.imwrite(f\"new_labels/{filename}_label_coloured.tif\",rgba_img.to(torch.uint8).numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee34c14d-9f1b-4b5d-a1b8-cf177ea00022",
   "metadata": {},
   "source": [
    "# Manually fixed:\n",
    "\n",
    "- Mix 10_013\n",
    "- Mix 2_000\n",
    "- Mix 2_001\n",
    "- Mix 2_002\n",
    "- Mix 2_003\n",
    "- Mix 2_004\n",
    "- Mix 2_005\n",
    "- Mix 2_006\n",
    "- Mix 2_007\n",
    "- Mix 2_008\n",
    "- Mix 2_009\n",
    "- Mix 2_010\n",
    "- Mix 2_011\n",
    "- Mix 2_012\n",
    "- Mix 2_013\n",
    "- Mix 3_000\n",
    "- Mix 3_001\n",
    "- Mix 3_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d513e92a-ac9e-4f07-979b-f24c5bf63c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 321\n",
    "img,_ = test_loader.dataset.__getitem__(i)\n",
    "img = img.unsqueeze(dim=0)\n",
    "new_labels = np.zeros_like(img)\n",
    "filename = test_loader.dataset.get_name_of_image(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d064c557-2016-4da0-9b11-aa6ff1cff0a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "i = 6\n",
    "img,_ = test_loader.dataset.__getitem__(i)\n",
    "img = img.unsqueeze(dim=0)\n",
    "new_labels = np.zeros_like(img)\n",
    "filename = test_loader.dataset.get_name_of_image(i)\n",
    "\n",
    "files = glob(f\"outputs_SwinUNETR/{filename}_acc=*.tif\")\n",
    "for file in files:\n",
    "    if \"coloured\" in file:\n",
    "        labels = tifffile.imread(file)\n",
    "    else:\n",
    "        correct = tifffile.imread(file)\n",
    "        \n",
    "# correct[70:110,57:100,25:50,:][(labels[70:110,57:100,25:50,0] == 255) & (labels[70:110,57:100,25:50,1] == 0) & (labels[70:110,57:100,25:50,2] == 0)] = [0,128,0,128]\n",
    "# correct[70:110,57:100,25:50,:][(labels[70:110,57:100,25:50,0] == 255) & (labels[70:110,57:100,25:50,1] == 0) & (labels[70:110,57:100,25:50,2] == 255)] = [0,128,0,128]\n",
    "# correct[70:110,57:100,25:50,:][(labels[70:110,57:100,25:50,0] == 161) & (labels[70:110,57:100,25:50,1] == 161) & (labels[70:110,57:100,25:50,2] == 255)] = [0,128,0,128]\n",
    "# labels[70:110,57:100,25:50,:][(labels[70:110,57:100,25:50,0] == 255) & (labels[70:110,57:100,25:50,1] == 0) & (labels[70:110,57:100,25:50,2] == 0)] = [0,255,0,128]\n",
    "# labels[70:110,57:100,25:50,:][(labels[70:110,57:100,25:50,0] == 255) & (labels[70:110,57:100,25:50,1] == 0) & (labels[70:110,57:100,25:50,2] == 255)] = [0,255,0,128]\n",
    "# labels[70:110,57:100,25:50,:][(labels[70:110,57:100,25:50,0] == 161) & (labels[70:110,57:100,25:50,1] == 161) & (labels[70:110,57:100,25:50,2] == 255)] = [0,255,0,128]\n",
    "\n",
    "# correct[92:112,86:106,87:101,:][(labels[92:112,86:106,87:101,0] == 255) & (labels[92:112,86:106,87:101,1] == 0) & (labels[92:112,86:106,87:101,2] == 255)] = [0,128,0,128]\n",
    "# correct[92:112,86:106,87:101,:][(labels[92:112,86:106,87:101,0] == 255) & (labels[92:112,86:106,87:101,1] == 255) & (labels[92:112,86:106,87:101,2] == 0)] = [0,128,0,128]\n",
    "# correct[92:112,86:106,87:101,:][(labels[92:112,86:106,87:101,0] == 255) & (labels[92:112,86:106,87:101,1] == 0) & (labels[92:112,86:106,87:101,2] == 0)] = [0,128,0,128]\n",
    "# labels[92:112,86:106,87:101,:][(labels[92:112,86:106,87:101,0] == 255) & (labels[92:112,86:106,87:101,1] == 0) & (labels[92:112,86:106,87:101,2] == 255)] = [135,89,179,128]\n",
    "# labels[92:112,86:106,87:101,:][(labels[92:112,86:106,87:101,0] == 255) & (labels[92:112,86:106,87:101,1] == 255) & (labels[92:112,86:106,87:101,2] == 0)] = [135,89,179,128]\n",
    "# labels[92:112,86:106,87:101,:][(labels[92:112,86:106,87:101,0] == 255) & (labels[92:112,86:106,87:101,1] == 0) & (labels[92:112,86:106,87:101,2] == 0)] = [135,89,179,128]\n",
    "\n",
    "# correct[40:80,98:110,25:45,:][(labels[40:80,98:110,25:45,0] == 255) & (labels[40:80,98:110,25:45,1] == 128) & (labels[40:80,98:110,25:45,2] == 191)] = [0,128,0,128]\n",
    "# labels[40:80,98:110,25:45,:][(labels[40:80,98:110,25:45,0] == 255) & (labels[40:80,98:110,25:45,1] == 128) & (labels[40:80,98:110,25:45,2] == 191)] = [135,89,179,128]\n",
    "\n",
    "mask = correct[:,:,:,1] != 0\n",
    "\n",
    "temp = np.zeros((labels[mask].shape[0]))\n",
    "for item in color_dict.items():\n",
    "    temp_mask = np.all(labels[mask][:,:3] == item[1],axis=1)\n",
    "    temp[temp_mask] = item[0]\n",
    "new_labels[:,:,mask] = temp\n",
    "\n",
    "files = glob(f\"outputs_SwinUNETR2/{filename}_acc=*.tif\")\n",
    "for file in files:\n",
    "    if \"coloured\" in file:\n",
    "        labels = tifffile.imread(file)\n",
    "    else:\n",
    "        correct = tifffile.imread(file)\n",
    "mask = correct[:,:,:,1] != 0\n",
    "\n",
    "temp = np.zeros((labels[mask].shape[0]))\n",
    "for item in color_dict.items():\n",
    "    temp_mask = np.all(labels[mask][:,:3] == item[1],axis=1)\n",
    "    temp[temp_mask] = item[0]\n",
    "new_labels[:,:,mask] = temp\n",
    "\n",
    "files = glob(f\"outputs_big_data/{filename}_acc=*.tif\")\n",
    "for file in files:\n",
    "    if \"coloured\" in file:\n",
    "        labels = tifffile.imread(file)\n",
    "    else:\n",
    "        correct = tifffile.imread(file)\n",
    "\n",
    "correct[(labels[:,:,:,0] == 0) & (labels[:,:,:,1] == 85) & (labels[:,:,:,2] == 0)] = [0,128,0,128]\n",
    "# 220 Mix 3_002\n",
    "# correct[(labels[:,:,:,0] == 255) & (labels[:,:,:,1] == 255)] = [0,128,0,128]\n",
    "# labels[(labels[:,:,:,0] == 255) & (labels[:,:,:,1] == 255)] = [171,128,84,128]\n",
    "\n",
    "# labels[(labels[:,:,:,1] == 255) & (labels[:,:,:,0] == 0) & (labels[:,:,:,2] == 0)] = [255,0,0,128]\n",
    "# correct[(labels[:,:,:,0] == 255) & (labels[:,:,:,1] == 0) & (labels[:,:,:,2] == 0)] = [0,128,0,128]\n",
    "\n",
    "# 186 Mix 2_006\n",
    "# correct[70:110,57:100,25:50,:][(labels[70:110,57:100,25:50,0] == 255) & (labels[70:110,57:100,25:50,1] == 0) & (labels[70:110,57:100,25:50,2] == 0)] = [0,128,0,128]\n",
    "# correct[70:110,57:100,25:50,:][(labels[70:110,57:100,25:50,0] == 255) & (labels[70:110,57:100,25:50,1] == 255) & (labels[70:110,57:100,25:50,2] == 0)] = [0,128,0,128]\n",
    "# correct[112:155,:40,25:70,:][(labels[112:155,:40,25:70,0] == 255) & (labels[112:155,:40,25:70,1] == 0) & (labels[112:155,:40,25:70,2] == 0)] = [0,128,0,128]\n",
    "# correct[112:155,:40,25:70,:][(labels[112:155,:40,25:70,0] == 255) & (labels[112:155,:40,25:70,1] == 255) & (labels[112:155,:40,25:70,2] == 0)] = [0,128,0,128]\n",
    "# correct[:,:,90:,:][(labels[:,:,90:,0] == 255) & (labels[:,:,90:,1] == 0) & (labels[:,:,90:,2] == 0)] = [0,128,0,128]\n",
    "\n",
    "# labels[70:110,57:100,25:50,:][(labels[70:110,57:100,25:50,0] == 255) & (labels[70:110,57:100,25:50,1] == 0) & (labels[70:110,57:100,25:50,2] == 0)] = [0,255,0,128]\n",
    "# labels[70:110,57:100,25:50,:][(labels[70:110,57:100,25:50,0] == 255) & (labels[70:110,57:100,25:50,1] == 255) & (labels[70:110,57:100,25:50,2] == 0)] = [0,255,0,128]\n",
    "# labels[112:155,:40,25:70,:][(labels[112:155,:40,25:70,0] == 255) & (labels[112:155,:40,25:70,1] == 0) & (labels[112:155,:40,25:70,2] == 0)] = [0,255,0,128]\n",
    "# labels[112:155,:40,25:70,:][(labels[112:155,:40,25:70,0] == 255) & (labels[112:155,:40,25:70,1] == 255) & (labels[112:155,:40,25:70,2] == 0)] = [0,255,0,128]\n",
    "# labels[:,:,90:,:][(labels[:,:,90:,0] == 255) & (labels[:,:,90:,1] == 0) & (labels[:,:,90:,2] == 0)] = [0,255,0,128]\n",
    "\n",
    "mask = correct[:,:,:,1] != 0\n",
    "\n",
    "temp = np.zeros((labels[mask].shape[0]))\n",
    "for item in color_dict.items():\n",
    "    temp_mask = np.all(labels[mask][:,:3] == item[1],axis=1)\n",
    "    temp[temp_mask] = item[0]\n",
    "new_labels[:,:,mask] = temp\n",
    "tifffile.imwrite(f\"new_labels/{filename}_label.tif\",new_labels.astype(\"uint8\"))\n",
    "\n",
    "rgba_img = torch.zeros(256,128,128,4)\n",
    "for i in range(0,13):\n",
    "    if i == 0:\n",
    "        rgba_img[:,:,:,3][(new_labels == i).squeeze()] = 255\n",
    "    else:\n",
    "        rgba_img[:,:,:,0][(new_labels == i).squeeze()] = color_dict[i][0]\n",
    "        rgba_img[:,:,:,1][(new_labels == i).squeeze()] = color_dict[i][1]\n",
    "        rgba_img[:,:,:,2][(new_labels == i).squeeze()] = color_dict[i][2]\n",
    "        rgba_img[:,:,:,3][(new_labels == i).squeeze()] = 128\n",
    "tifffile.imwrite(f\"new_labels/{filename}_label_coloured.tif\",rgba_img.to(torch.uint8).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "af23b0bc-1a46-4449-852a-bd201e60674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import _PATH_DATA, _PATH_MODELS, _PROJECT_ROOT\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dbdf7b68-6c9e-4284-8d61-6c0aa06048b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Mix 2_006\"\n",
    "new_labels = tifffile.imread(f\"{filename}_label.tif\")\n",
    "new_labels = new_labels.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0ede35ec-72e8-495e-9c7e-37fdb1f5d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tifffile.imwrite(f\"../data/fixed_labels/{filename}_label.tif\",new_labels.astype(\"uint8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a78fb0a3-f304-45c1-b975-e03e5f81d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgba_img = torch.zeros(256,128,128,4)\n",
    "for i in range(0,13):\n",
    "    if i == 0:\n",
    "        rgba_img[:,:,:,3][(new_labels == i).squeeze()] = 255\n",
    "    else:\n",
    "        rgba_img[:,:,:,0][(new_labels == i).squeeze()] = color_dict[i][0]\n",
    "        rgba_img[:,:,:,1][(new_labels == i).squeeze()] = color_dict[i][1]\n",
    "        rgba_img[:,:,:,2][(new_labels == i).squeeze()] = color_dict[i][2]\n",
    "        rgba_img[:,:,:,3][(new_labels == i).squeeze()] = 128\n",
    "tifffile.imwrite(f\"../data/fixed_labels/{filename}_label_coloured.tif\",rgba_img.to(torch.uint8).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9fca0db-ea2c-48f5-ad24-3db9e28f6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import _PATH_DATA, _PATH_MODELS, _PROJECT_ROOT\n",
    "from src.data.dataloaders import MetricDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import scipy.ndimage as ndi\n",
    "from src.data.dataloaders import Label\n",
    "\n",
    "color_dict = {1:[255,0,0],2:[0,255,0],3:[0,0,255],4:[255,255,0],5:[255,0,255],6:[0,255,255],7:[161,161,255],8:[171,128,84],9:[255,128,191],10:[135,89,179],11:[255,191,128],12:[0,85,0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8033f0c-805d-47c7-b899-d211e67c9f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/mixed_and_label_paths.csv\")\n",
    "label_paths = df.label_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b840554-d2b0-42db-8e1c-dfed7a70ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_path in label_paths:\n",
    "    filename = \"_\".join(label_path.split(\"/\")[-1].split(\"_\")[:2])\n",
    "    \n",
    "    files = glob(f\"outputs_SwinUNETR/{filename}_acc=*.tif\")\n",
    "    for file in files:\n",
    "        if \"coloured\" not in file:\n",
    "            swin1 = tifffile.imread(file)\n",
    "    files = glob(f\"outputs_SwinUNETR2/{filename}_acc=*.tif\")\n",
    "    for file in files:\n",
    "        if \"coloured\" not in file:\n",
    "            swin2 = tifffile.imread(file)\n",
    "    \n",
    "    files = glob(f\"outputs_big_data/{filename}_acc=*.tif\")\n",
    "    for file in files:\n",
    "        if \"coloured\" not in file:\n",
    "            unet = tifffile.imread(file)\n",
    "\n",
    "    img = np.zeros((256,128,128))\n",
    "    img[swin1[:,:,:,3] == 128] = 1\n",
    "    img[swin2[:,:,:,3] == 128] = 1\n",
    "    img[unet[:,:,:,3] == 128] = 1\n",
    "\n",
    "    label = tifffile.imread(f\"{_PATH_DATA}/{label_path}\")\n",
    "\n",
    "    img[img == 1] = label[img == 1]\n",
    "    img = img.astype(\"uint8\")\n",
    "    tifffile.imwrite(f\"new_labels2/{filename}_label.tif\",img)\n",
    "\n",
    "    rgba_img = torch.zeros(256,128,128,4)\n",
    "    for i in range(0,13):\n",
    "        if i == 0:\n",
    "            rgba_img[:,:,:,3][(img == i).squeeze()] = 255\n",
    "        else:\n",
    "            rgba_img[:,:,:,0][(img == i).squeeze()] = color_dict[i][0]\n",
    "            rgba_img[:,:,:,1][(img == i).squeeze()] = color_dict[i][1]\n",
    "            rgba_img[:,:,:,2][(img == i).squeeze()] = color_dict[i][2]\n",
    "            rgba_img[:,:,:,3][(img == i).squeeze()] = 128\n",
    "    tifffile.imwrite(f\"new_labels2/{filename}_label_coloured.tif\",rgba_img.to(torch.uint8).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb1dbf-4bb3-4c94-9ee4-192c541b417f",
   "metadata": {},
   "source": [
    "# Systematic test what is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fee552-6da9-4138-b0bd-c8c26cca8aec",
   "metadata": {},
   "source": [
    "| Model:       | UNet_small |       |       | UNet_large |       |       | SwinUNETR |       |       |\n",
    "|--------------|:----------:|:-----:|:-----:|:----------:|:-----:|:-----:|:---------:|:-----:|:-----:|\n",
    "| dataset/size |    5000    | 20000 | 50000 |    5000    | 20000 | 50000 |    5000   | 20000 | 50000 |\n",
    "|      v1      |          17.3 |     x |     x |          x |     x |     x |         x |     x |     x |\n",
    "|      v2      |          x |     x |     x |          x |     x |     x |         x |     x |     x |\n",
    "|      v3      |          x |     x |     x |          x |     x |     x |         x |     x |     x |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c178c3-ca61-4369-b370-51d66d92b548",
   "metadata": {},
   "source": [
    "| Transformation\\Model: | UNet_small | UNet_large | SwinUNETR |\r\n",
    "|-----------------------|:----------:|:----------:|:---------:|\r\n",
    "|          UMAP         |          x |          x |         x |\r\n",
    "|          PCA          |          x |          x |         x |\r\n",
    "|  Raw feature distance |          x |          x |         x |\r\n",
    "|          GAN          |          x |          x |         x |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "588e9d59-e463-43f7-9f18-3e7821db85f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.dataloaders import BugNISTDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2857fca0-7b51-46e6-97d3-1a72dc56cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "274035d6-99e7-4826-b3af-32f06dac76ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugnist = BugNISTDataModule(batch_size=1, num_workers=0, mix=True)\n",
    "bugnist.setup()\n",
    "\n",
    "loader = bugnist.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec4dbe44-615c-4109-83bb-713f18e546c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c863b6ed-0c08-4c8b-a207-1227092dd667",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    img,label = next(iterator)\n",
    "    tifffile.imwrite(f\"img_{i}.tif\",img.to(torch.uint8).numpy())\n",
    "    tifffile.imwrite(f\"label_{i}.tif\",label.to(torch.uint8).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b650b9-8b4f-49e8-abdd-7709e4f471a5",
   "metadata": {},
   "source": [
    "# Testing on fixed labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c76ba3-2b05-4545-aec6-f2720ce9b3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    }
   ],
   "source": [
    "from src import _PATH_DATA, _PATH_MODELS, _PROJECT_ROOT\n",
    "from src.models.unet import UNet_pl\n",
    "from src.models.swin_unetr import SwinUNETR_pl as SwinUNETR\n",
    "import torch\n",
    "import torch._dynamo\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tifffile\n",
    "from skimage import measure\n",
    "import numpy as np\n",
    "from src.data.dataloaders import MetricDataset#, BugNISTDataModule\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04efecb2-6660-46f7-8b5b-c6d7706415b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = glob(\"fixed_labels/*label.tif\")\n",
    "# labels.sort()\n",
    "\n",
    "# img = [\"real_256/train/00/Mix 10_013.tif\",\n",
    "# \"real_256/validation/01/Mix 2_000.tif\",\n",
    "# \"real_256/validation/00/Mix 2_001.tif\",\n",
    "# \"real_256/train/01/Mix 2_002.tif\",\n",
    "# \"real_256/train/01/Mix 2_003.tif\",\n",
    "# \"real_256/train/01/Mix 2_004.tif\",\n",
    "# \"real_256/validation/00/Mix 2_005.tif\",\n",
    "# \"real_256/train/01/Mix 2_006.tif\",\n",
    "# \"real_256/train/01/Mix 2_007.tif\",\n",
    "# \"real_256/train/01/Mix 2_008.tif\",\n",
    "# \"real_256/train/01/Mix 2_009.tif\",\n",
    "# \"real_256/train/01/Mix 2_010.tif\",\n",
    "# \"real_256/train/01/Mix 2_011.tif\",\n",
    "# \"real_256/train/01/Mix 2_012.tif\",\n",
    "# \"real_256/validation/00/Mix 2_013.tif\",\n",
    "# \"real_256/train/02/Mix 3_000.tif\",\n",
    "# \"real_256/train/02/Mix 3_001.tif\",\n",
    "# \"real_256/train/02/Mix 3_002.tif\",]\n",
    "\n",
    "# df = pd.DataFrame(data={\"img_path\":img,\"label_path\":labels}\n",
    "# df.to_csv(\"fixed_labels.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d793a1-cc12-42cd-a77d-ef58a11db738",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "test_loader = DataLoader(MetricDataset(fixed=True),\n",
    "                     batch_size=1,\n",
    "                     num_workers=0)\n",
    "\n",
    "\n",
    "# model = UNet_pl(\n",
    "#     spatial_dims=3,\n",
    "#     in_channels=1,\n",
    "#     out_channels=13,\n",
    "#     channels=(4, 8, 16, 32, 64),\n",
    "#     strides=(2, 2, 2, 2),\n",
    "#     lr=1,\n",
    "# )\n",
    "\n",
    "model = UNet_pl(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=13,\n",
    "        channels=(16, 32, 64, 128, 256, 512),\n",
    "        strides=(2, 2, 2, 2, 2),\n",
    "        num_res_units = 3,\n",
    "    )\n",
    "\n",
    "# model = SwinUNETR(img_size=(256,128,128), in_channels=1, out_channels=13, feature_size=24)\n",
    "\n",
    "# model_path = glob(f\"models/{args.model}_{args.version}*/*.ckpt\")[0]\n",
    "model_path = glob(\"../models/large50000v3*/*.ckpt\")[0]\n",
    "\n",
    "model.load_state_dict(torch.load(model_path, map_location=None)['state_dict'], strict=True)\n",
    "    \n",
    "torch._dynamo.config.suppress_errors = True\n",
    "model = torch.compile(model)\n",
    "model.eval();\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd687fa-1da4-42f5-9425-d706ad657f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "for k,(img,label) in enumerate(tqdm(test_loader, unit=\"batch\")):\n",
    "    img = img.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        preds = model(img);\n",
    "        preds_sm = preds.softmax(dim=1).cpu().numpy();\n",
    "\n",
    "        class_props = []\n",
    "        for i in range(1, 13):  # For each class channel\n",
    "            props = measure.regionprops(\n",
    "        \n",
    "                #                        v-- softmax channel for this class\n",
    "        \n",
    "                measure.label(preds_sm[0, i] > 0.25)  # Threshold softmax probability at 0.25\n",
    "        \n",
    "            )\n",
    "        \n",
    "            props = [p for p in props if p.area > 5**3]  # Remove small connected components\n",
    "        \n",
    "            class_props.append(props)\n",
    "        \n",
    "        for i, props in enumerate(class_props):\n",
    "            # For every found connected component\n",
    "            for p in props:\n",
    "                bb = p.bbox\n",
    "        \n",
    "                # Sets the found connected component to the mean value of the connected component\n",
    "                preds_sm[0,i+1,bb[0]:bb[3],bb[1]:bb[4],bb[2]:bb[5]][p.image] = preds_sm[0,i+1,bb[0]:bb[3],bb[1]:bb[4],bb[2]:bb[5]][p.image].mean()\n",
    "        \n",
    "        out = torch.Tensor(preds_sm).softmax(dim=1).argmax(dim=1).to(torch.uint8)\n",
    "        accuracies.append(sum(out[out != 0] == label[out != 0])/len(label[out != 0]))\n",
    "accuracies = torch.Tensor(accuracies)\n",
    "print(accuracies.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4e3a91d-8bc7-4546-8b71-c0f67bf5efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e280fab4-4a3b-4f58-adb9-43be487ea674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/large50000v2-2024-02-08-1000/UNet_large-epoch=25.ckpt',\n",
       " '../models/large50000v2-2024-02-01-0711/UNet_large-epoch=46.ckpt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(f\"../models/large50000v2*/*.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645035a1-8e80-44e5-9e8e-927c38153bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison = torch.zeros(label.shape)\n",
    "        \n",
    "        # comparison[out == label] = 2 # Finds correctly labelled pixels\n",
    "        # comparison[out != label] = 1 # Finds wrongly labelled pixels\n",
    "        # comparison[out == 0] = 0    # Removes background pixels\n",
    "        \n",
    "        \n",
    "        # rgba_img = torch.zeros(256,128,128,4)\n",
    "        \n",
    "        # rgba_img[:,:,:,1][comparison[0] == 2] = 128 # Set colour of correctly labelled pixels to green\n",
    "        # rgba_img[:,:,:,3][comparison[0] == 2] = 128 # Set transparency of correctly labelled pixels\n",
    "        # rgba_img[:,:,:,0][comparison[0] == 1] = 128 # Set colour of wrongly labelled pixels to red\n",
    "        # rgba_img[:,:,:,3][comparison[0] == 1] = 128 # Set transparency of wrongly labelled pixels\n",
    "        # rgba_img[:,:,:,3][comparison[0] == 0] = 255 # Set transparency of background (fully transparent)\n",
    "        # tifffile.imwrite(f\"outputs_SwinUNETR2/{test_loader.dataset.get_name_of_image(k)}_acc={accuracies[k]:.3f}.tif\",rgba_img.to(torch.uint8).numpy())\n",
    "\n",
    "        # rgba_img = torch.zeros(256,128,128,4)\n",
    "        # for i in range(0,13):\n",
    "        #     if i == 0:\n",
    "        #         rgba_img[:,:,:,3][(out == i).squeeze()] = 255\n",
    "        #     else:\n",
    "        #         rgba_img[:,:,:,0][(out == i).squeeze()] = color_dict[i][0]\n",
    "        #         rgba_img[:,:,:,1][(out == i).squeeze()] = color_dict[i][1]\n",
    "        #         rgba_img[:,:,:,2][(out == i).squeeze()] = color_dict[i][2]\n",
    "        #         rgba_img[:,:,:,3][(out == i).squeeze()] = 128\n",
    "        # tifffile.imwrite(f\"outputs_SwinUNETR2/{test_loader.dataset.get_name_of_image(k)}_acc={accuracies[k]:.3f}_coloured.tif\",rgba_img.to(torch.uint8).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc2b21a-f238-4404-b8f3-63fd3103f4e3",
   "metadata": {},
   "source": [
    "# Testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89f45c13-151a-402f-81af-d147770f5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import _PATH_DATA, _PATH_MODELS, _PROJECT_ROOT\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bddb141-334a-4947-a4f8-06c493813a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {1:[255,0,0],2:[0,255,0],3:[0,0,255],4:[255,255,0],5:[255,0,255],6:[0,255,255],7:[161,161,255],8:[171,128,84],9:[255,128,191],10:[135,89,179],11:[255,191,128],12:[0,85,0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48759169-490a-4e69-b362-b0928ac51f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = tifffile.imread(f\"{_PATH_DATA}/synthetic_mixed_256_v3/train/000/label_07100.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e271b47e-8aa9-44ad-96c1-fb7061988b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tifffile\n",
    "from py3dbp import Packer, Bin, Item\n",
    "import re\n",
    "import numpy as np\n",
    "from src.data.make_dataset import Label\n",
    "from src import _PROJECT_ROOT, _PATH_DATA\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from glob import glob\n",
    "import scipy.ndimage as ndi\n",
    "from scipy.special import softmax\n",
    "\n",
    "rng = np.random.RandomState(seed=199742)\n",
    "\n",
    "labels=[\"BF\",\"CF\",\"PP\",\"MA\",\"BL\",\"ML\",\"SL\",\"WO\",\"BC\",\"GH\",\"AC\",\"GP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93c46bdc-6f80-4a4e-874d-cafe389380de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin(df, composition, ax0_bottom, ax0_top, ax1_bottom, ax1_top, ax2_bottom, ax2_top):\n",
    "    packer = Packer()\n",
    "    packer.add_bin(Bin(f\"mix\", ax0_top-ax0_bottom, ax1_top-ax1_bottom, ax2_top-ax2_bottom, 999))\n",
    "    \n",
    "    for label in composition:\n",
    "        df_sample = df[df[\"label\"]==label].sample(n=1,random_state=rng)\n",
    "        shape = [int(s) for s in re.findall(r\"\\d+\", df_sample[\"size\"].item())]\n",
    "        packer.add_item(Item(df_sample.filename.item(), shape[0], shape[1], shape[2], 1))\n",
    "\n",
    "    packer.pack(bigger_first=bool(np.random.randint(2)))\n",
    "\n",
    "    bin = next(iter(packer.bins))\n",
    "\n",
    "    return bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd5ef84-3397-4021-b277-9be1f9e5e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_new_starting_position(start_position, temp_img_shape, temp_img_label, new_mix_label, ax0_bottom, ax0_top, ax1_bottom, ax1_top, ax2_bottom, ax2_top):\n",
    "    shift_0 = int(start_position[0])\n",
    "    working_shift_0 = shift_0\n",
    "    shift_1 = int(start_position[1])\n",
    "    working_shift_1 = shift_1\n",
    "    shift_2 = int(start_position[2])\n",
    "    working_shift_2 = shift_2\n",
    "    \n",
    "    \n",
    "    if shift_0 != 0:\n",
    "        i = 2\n",
    "        while True:\n",
    "            shift_0 = working_shift_0 - int(shift_0/i)\n",
    "                \n",
    "            if np.any(new_mix_label[\n",
    "                      ax0_bottom+shift_0 : ax0_bottom+shift_0 + temp_img_shape[0],\n",
    "                      ax1_bottom+shift_1 : ax1_bottom+shift_1 + temp_img_shape[1],\n",
    "                      ax2_bottom+shift_2 : ax2_bottom+shift_2 + temp_img_shape[2],\n",
    "                      ][temp_img_label != 0] != 0):\n",
    "                shift_0 = working_shift_0\n",
    "                i = i*2\n",
    "                if shift_0/i < 0.1:\n",
    "                    break\n",
    "            else:\n",
    "                if shift_0 >= working_shift_0:\n",
    "                    break\n",
    "                elif shift_0 == 0:\n",
    "                    working_shift_0 = shift_0\n",
    "                    break\n",
    "                working_shift_0 = shift_0\n",
    "\n",
    "    if shift_1 != 0:\n",
    "        i = 2\n",
    "        while True:\n",
    "            shift_1 = working_shift_1 - int(shift_1/i)\n",
    "                \n",
    "            if np.any(new_mix_label[\n",
    "                      ax0_bottom+shift_0 : ax0_bottom+shift_0 + temp_img_shape[0],\n",
    "                      ax1_bottom+shift_1 : ax1_bottom+shift_1 + temp_img_shape[1],\n",
    "                      ax2_bottom+shift_2 : ax2_bottom+shift_2 + temp_img_shape[2],\n",
    "                      ][temp_img_label != 0] != 0):\n",
    "                shift_1 = working_shift_1\n",
    "                i = i*2\n",
    "                if shift_1/i < 0.1:\n",
    "                    break\n",
    "            else:\n",
    "                if shift_1 >= working_shift_1:\n",
    "                    break\n",
    "                elif shift_1 == 0:\n",
    "                    working_shift_1 = shift_1\n",
    "                    break\n",
    "                working_shift_1 = shift_1\n",
    "\n",
    "    if shift_2 != 0:\n",
    "        i = 2\n",
    "        while True:\n",
    "            shift_2 = working_shift_2 - int(shift_2/i)\n",
    "            \n",
    "            if np.any(new_mix_label[\n",
    "                      ax0_bottom+shift_0 : ax0_bottom+shift_0 + temp_img_shape[0],\n",
    "                      ax1_bottom+shift_1 : ax1_bottom+shift_1 + temp_img_shape[1],\n",
    "                      ax2_bottom+shift_2 : ax2_bottom+shift_2 + temp_img_shape[2],\n",
    "                      ][temp_img_label != 0] != 0):\n",
    "                shift_2 = working_shift_2\n",
    "                i = i*2\n",
    "                if shift_2/i < 0.1:\n",
    "                    break\n",
    "                    \n",
    "            else:\n",
    "                if shift_2 >= working_shift_2:\n",
    "                    break\n",
    "                elif shift_2 == 0:\n",
    "                    working_shift_2 = shift_2\n",
    "                    break\n",
    "                working_shift_2 = shift_2\n",
    "                \n",
    "    return [working_shift_0,working_shift_1,working_shift_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "345991c1-9a32-48ff-b64d-76734670962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mix(counter, bin, i, mode, new_mix, new_mix_label, df, ax0_bottom, ax0_top, ax1_bottom, ax1_top, ax2_bottom, ax2_top):\n",
    "    skipped = 0\n",
    "    all_skipped = False\n",
    "    counter2 = 0\n",
    "    for item in bin.items:\n",
    "        temp_img = tifffile.imread(\n",
    "                \"/\".join([_PATH_DATA, \"bugnist_256_cut\", item.name])\n",
    "            )\n",
    "        if item.rotation_type == 1:\n",
    "            temp_img = np.swapaxes(temp_img, 0, 1)\n",
    "        elif item.rotation_type == 2:\n",
    "            temp_img = np.rollaxis(temp_img, 0, 3)\n",
    "        elif item.rotation_type == 3:\n",
    "            temp_img = np.swapaxes(temp_img, 0, 2)\n",
    "        elif item.rotation_type == 4:\n",
    "            temp_img = np.rollaxis(temp_img, 2, 0)\n",
    "        elif item.rotation_type == 5:\n",
    "            temp_img = np.swapaxes(temp_img, 1, 2)\n",
    "\n",
    "        temp_img_shape = temp_img.shape\n",
    "        start_position = item.position\n",
    "\n",
    "        temp_img_label = np.zeros(temp_img_shape, dtype=\"uint8\")\n",
    "        temp_img_label[temp_img >= 100] = Label.from_abbreviation(\n",
    "            item.name.split(\"/\")[0]\n",
    "        ).value\n",
    "\n",
    "        if np.any(new_mix_label[\n",
    "                      ax0_bottom+int(start_position[0]) : ax0_bottom+int(start_position[0]) + temp_img_shape[0],\n",
    "                      ax1_bottom+int(start_position[1]) : ax1_bottom+int(start_position[1]) + temp_img_shape[1],\n",
    "                      ax2_bottom+int(start_position[2]) : ax2_bottom+int(start_position[2]) + temp_img_shape[2],\n",
    "                      ][temp_img_label != 0] != 0):\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        new_mix_copy = new_mix.copy()\n",
    "        \n",
    "        new_mix_copy[\n",
    "        ax0_bottom+int(start_position[0]) : ax0_bottom+int(start_position[0]) + temp_img_shape[0],\n",
    "        ax1_bottom+int(start_position[1]) : ax1_bottom+int(start_position[1]) + temp_img_shape[1],\n",
    "        ax2_bottom+int(start_position[2]) : ax2_bottom+int(start_position[2]) + temp_img_shape[2],\n",
    "        ] = np.maximum(temp_img,new_mix_copy[\n",
    "        ax0_bottom+int(start_position[0]) : ax0_bottom+int(start_position[0]) + temp_img_shape[0],\n",
    "        ax1_bottom+int(start_position[1]) : ax1_bottom+int(start_position[1]) + temp_img_shape[1],\n",
    "        ax2_bottom+int(start_position[2]) : ax2_bottom+int(start_position[2]) + temp_img_shape[2],\n",
    "        ])\n",
    "        \n",
    "        tifffile.imwrite(f\"{counter}_before_{counter2}.tif\",new_mix_copy)\n",
    "        \n",
    "        [working_shift_0,working_shift_1,working_shift_2] = find_new_starting_position(start_position, temp_img_shape, temp_img_label, new_mix_label, ax0_bottom, ax0_top, ax1_bottom, ax1_top, ax2_bottom, ax2_top)\n",
    "\n",
    "        new_mix[\n",
    "        ax0_bottom+working_shift_0 : ax0_bottom+working_shift_0 + temp_img_shape[0],\n",
    "        ax1_bottom+working_shift_1 : ax1_bottom+working_shift_1 + temp_img_shape[1],\n",
    "        ax2_bottom+working_shift_2 : ax2_bottom+working_shift_2 + temp_img_shape[2],\n",
    "        ] = np.maximum(temp_img,new_mix[\n",
    "        ax0_bottom+working_shift_0 : ax0_bottom+working_shift_0 + temp_img_shape[0],\n",
    "        ax1_bottom+working_shift_1 : ax1_bottom+working_shift_1 + temp_img_shape[1],\n",
    "        ax2_bottom+working_shift_2 : ax2_bottom+working_shift_2 + temp_img_shape[2],\n",
    "        ])\n",
    "\n",
    "        \n",
    "        new_mix_label[\n",
    "        ax0_bottom+working_shift_0 : ax0_bottom+working_shift_0 + temp_img_shape[0],\n",
    "        ax1_bottom+working_shift_1 : ax1_bottom+working_shift_1 + temp_img_shape[1],\n",
    "        ax2_bottom+working_shift_2 : ax2_bottom+working_shift_2 + temp_img_shape[2],\n",
    "        ] = np.maximum(temp_img_label,new_mix_label[\n",
    "        ax0_bottom+working_shift_0 : ax0_bottom+working_shift_0 + temp_img_shape[0],\n",
    "        ax1_bottom+working_shift_1 : ax1_bottom+working_shift_1 + temp_img_shape[1],\n",
    "        ax2_bottom+working_shift_2 : ax2_bottom+working_shift_2 + temp_img_shape[2],\n",
    "        ])\n",
    "\n",
    "\n",
    "        estimated_centroid = ndi.center_of_mass(temp_img,\n",
    "                                                temp_img_label,\n",
    "                                                Label.from_abbreviation(\n",
    "                                                    item.name.split(\"/\")[0]).value)\n",
    "        \n",
    "        df.loc[len(df)] = [item.name.split(\"/\")[0],\n",
    "                           ax2_bottom+working_shift_2+estimated_centroid[2],\n",
    "                           ax1_bottom+working_shift_1+estimated_centroid[1],\n",
    "                           ax0_bottom+working_shift_0+estimated_centroid[0]]\n",
    "        \n",
    "        tifffile.imwrite(f\"{counter}_{counter2}.tif\",new_mix)\n",
    "        counter2 += 1\n",
    "\n",
    "    if skipped == len(bin.items):\n",
    "        all_skipped = True\n",
    "\n",
    "    return new_mix, new_mix_label, df, all_skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c2fd32-59ea-4a46-be25-7a13bc86ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mix(df, noise, i, mode, L, with_noise=True):\n",
    "    df = df[df[mode] == True].reset_index(drop=True)\n",
    "\n",
    "    if with_noise:\n",
    "        new_mix = tifffile.imread(rng.choice(noise))\n",
    "    else:\n",
    "        new_mix = np.zeros((256, 128, 128), dtype=\"uint8\")\n",
    "    \n",
    "    \n",
    "    ax2_top = new_mix.shape[2]\n",
    "    ax2_bottom = 0\n",
    "    ax1_top = new_mix.shape[1]\n",
    "    ax1_bottom = 0\n",
    "    ax0_top = new_mix.shape[0]\n",
    "    ax0_bottom = 0\n",
    "\n",
    "    if with_noise:\n",
    "        for i in range(new_mix.shape[2]):\n",
    "            if np.any(new_mix[:, :, i] != 0):\n",
    "                ax2_bottom = i\n",
    "                break\n",
    "        for i in range(new_mix.shape[2] - 1, -1, -1):\n",
    "            if np.any(new_mix[:, :, i] != 0):\n",
    "                ax2_top = i\n",
    "                break\n",
    "        for i in range(new_mix.shape[1]):\n",
    "            if np.any(new_mix[:, i, :] != 0):\n",
    "                ax1_bottom = i\n",
    "                break\n",
    "        for i in range(new_mix.shape[1] - 1, -1, -1):\n",
    "            if np.any(new_mix[:, i, :] != 0):\n",
    "                ax1_top = i\n",
    "                break\n",
    "        for i in range(new_mix.shape[0]):\n",
    "            if np.any(new_mix[i, :, :] != 0):\n",
    "                ax0_bottom = i\n",
    "                break\n",
    "        for i in range(new_mix.shape[0] - 1, -1, -1):\n",
    "            if np.any(new_mix[i, :, :] != 0):\n",
    "                ax0_top = i\n",
    "                break\n",
    "\n",
    "    new_mix_label = np.zeros((256, 128, 128), dtype=\"uint8\")\n",
    "    df_new = pd.DataFrame(columns=[\"Caption\",\"PosX\",\"PosY\",\"PosZ\"])\n",
    "    \n",
    "    if len(L) != 0:\n",
    "        prob=[]\n",
    "        for name in labels:\n",
    "            prob.append(1-(L.count(name)*50)/len(L))\n",
    "        prob = softmax(prob)\n",
    "    else:\n",
    "        prob=[1/12]*12\n",
    "\n",
    "    counter = 0\n",
    "    while True:\n",
    "        composition = rng.choice(labels,size=20,p=prob)\n",
    "        bin = get_bin(df, composition, ax0_bottom, ax0_top, ax1_bottom, ax1_top, ax2_bottom, ax2_top)\n",
    "    \n",
    "        new_mix, new_mix_label, df_new, all_skipped = create_mix(counter, bin, i, mode, new_mix, new_mix_label, df_new, ax0_bottom, ax0_top, ax1_bottom, ax1_top, ax2_bottom, ax2_top)\n",
    "        if all_skipped:\n",
    "            break\n",
    "        counter += 1\n",
    "            \n",
    "    L.extend(df_new.Caption.tolist())\n",
    "    return L, new_mix, new_mix_label, df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8fb8778-c7fb-4e08-b10d-9a3945021caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=1997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52f6d45f-97bf-41c4-bc2e-21c690d5c23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/\".join([_PATH_DATA, \"bugnist_256_cut/splits.csv\"]))\n",
    "df[\"label\"] = df.filename.apply(lambda x: x.split(\"/\")[0])\n",
    "noise = glob(_PATH_DATA + \"/noise/*.tif\")\n",
    "\n",
    "L = []\n",
    "L, new_mix, new_mix_label, df_new = generate_mix(df,noise,1,\"train\",L,False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2956f1-7692-4723-b7d3-86061d6e2441",
   "metadata": {},
   "source": [
    "# Create pseudo label creation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6827d468-d639-45fa-a373-efa9c5854b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndi\n",
    "import tifffile\n",
    "from src import _PATH_DATA\n",
    "from src.data.dataloaders import Label\n",
    "from skimage.morphology import skeletonize_3d\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def make_labels():\n",
    "    img_path = f\"{_PATH_DATA}/real_256/train/01/Mix 2_003.tif\"\n",
    "    label_path = f\"{_PATH_DATA}/mixed_crop/Mix 2/Mix 2_003_centroids.csv\"\n",
    "    df_label = pd.read_csv(label_path,sep=\";\")\n",
    "    df_label = df_label.round().astype({\"PosX\":\"int16\",\"PosY\":\"int16\",\"PosZ\":\"int16\"})\n",
    "    \n",
    "    df_label[\"PosX_resized\"] = (df_label.PosX/650)*128\n",
    "    df_label[\"PosY_resized\"] = (df_label.PosY/650)*128\n",
    "    df_label[\"PosZ_resized\"] = (df_label.PosZ/900)*256\n",
    "    df_label = df_label.round().astype({\"PosX_resized\":\"int16\",\"PosY_resized\":\"int16\",\"PosZ_resized\":\"int16\"})\n",
    "    img = tifffile.imread(img_path)\n",
    "\n",
    "\n",
    "    img2 = img.copy()\n",
    "    for i in range(10):\n",
    "        img2 = ndi.grey_closing(img2,structure=ndi.generate_binary_structure(3,2))\n",
    "    for i in range(3):\n",
    "        img2 = ndi.grey_erosion(img2,structure=ndi.generate_binary_structure(3,2))\n",
    "    for i in range(6):\n",
    "        img2 = ndi.grey_opening(img2,structure=ndi.generate_binary_structure(3,2))\n",
    "\n",
    "    step1 = img2.astype(\"uint8\").copy()\n",
    "    tifffile.imwrite(\"step1.tif\",img2.astype(\"uint8\"))\n",
    "    img2[img2 < 25] = 0\n",
    "    skeleton = skeletonize_3d(img2)\n",
    "    \n",
    "    markers = np.zeros(img.shape,dtype=\"int8\")\n",
    "    markers[skeleton < 75] = -1\n",
    "    markers[skeleton >= 75] = 0\n",
    "    \n",
    "    for item in df_label.iterrows():\n",
    "        for i in range(-7,8):\n",
    "            for j in range(-7,8):\n",
    "                for k in range(-7,8):\n",
    "                    markers[item[1].PosZ_resized+i,item[1].PosY_resized+k,item[1].PosX_resized+j] = Label.from_abbreviation(item[1].Caption).value\n",
    "                        \n",
    "    skeleton_label = ndi.watershed_ift(skeleton,markers,structure=ndi.generate_binary_structure(3,3))\n",
    "    skeleton_label[skeleton_label == -1] = 0\n",
    "    step2 = skeleton_label.astype(\"uint8\").copy()\n",
    "    # tifffile.imwrite(\"step2.tif\",skeleton_label.astype(\"uint8\"))\n",
    "\n",
    "    for item in df_label.iterrows():\n",
    "        for i in range(-7,8):\n",
    "            for j in range(-7,8):\n",
    "                for k in range(-7,8):\n",
    "                    skeleton_label[item[1].PosZ_resized+i,item[1].PosY_resized+k,item[1].PosX_resized+j] = Label.from_abbreviation(item[1].Caption).value\n",
    "    \n",
    "    \n",
    "    img2 = np.array(img2).astype(np.float32)\n",
    "    mn = np.min(img2)\n",
    "    mx = np.max(img2)\n",
    "    norm = (img2 - mn) * (1.0 / (mx - mn))\n",
    "    img2 = (norm*255).astype(\"uint8\")\n",
    "    \n",
    "    skeleton_label[img2 < 15] = -1\n",
    "    label_img = ndi.watershed_ift(img, skeleton_label,structure=ndi.generate_binary_structure(3,2))\n",
    "    label_img[label_img == -1] = 0\n",
    "    \n",
    "    for item in df_label.iterrows():\n",
    "        for i in range(-7,8):\n",
    "            for j in range(-7,8):\n",
    "                for k in range(-7,8):\n",
    "                    label_img[item[1].PosZ_resized+i,item[1].PosY_resized+k,item[1].PosX_resized+j] = Label.from_abbreviation(item[1].Caption).value\n",
    "\n",
    "    step3 = label_img.astype(\"uint8\").copy()\n",
    "    # tifffile.imwrite(\"step3.tif\",label_img.astype(\"uint8\"))\n",
    "\n",
    "    label_img = label_img.astype(\"int16\")\n",
    "    dists = np.zeros((13,256,128,128))\n",
    "    label_img[label_img == 0] = -1\n",
    "    for i in range(1,13):\n",
    "        label_img[label_img == i] = 0\n",
    "        dists[i,:,:,:] = ndi.distance_transform_cdt(label_img)\n",
    "        label_img[label_img == 0] = i\n",
    "\n",
    "    dists[0,:,:,:] = np.inf\n",
    "    dists[dists == -1] = np.inf\n",
    "    dists = np.argmin(dists,axis=0)\n",
    "\n",
    "    step4 = dists.astype(\"uint8\").copy()\n",
    "    # tifffile.imwrite(\"step4.tif\",label_img.astype(\"uint8\"))\n",
    "\n",
    "    return step1, step2, step3, step4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "565b038b-c2b8-478e-92aa-0d08263bbdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1, step2, step3, step4 = make_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b73a8240-49d6-41e4-85f3-d9533228f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "color_dict = {1:[255,0,0],2:[0,255,0],3:[0,0,255],4:[255,255,0],5:[255,0,255],6:[0,255,255],7:[161,161,255],8:[171,128,84],9:[255,128,191],10:[135,89,179],11:[255,191,128],12:[0,85,0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a42f38-8cbf-459d-a475-2e92abdc8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_color(new_labels,name):\n",
    "    rgba_img = torch.zeros(256,128,128,4)\n",
    "    for i in range(0,13):\n",
    "        if i == 0:\n",
    "            rgba_img[:,:,:,3][(new_labels == i).squeeze()] = 255\n",
    "        else:\n",
    "            rgba_img[:,:,:,0][(new_labels == i).squeeze()] = color_dict[i][0]\n",
    "            rgba_img[:,:,:,1][(new_labels == i).squeeze()] = color_dict[i][1]\n",
    "            rgba_img[:,:,:,2][(new_labels == i).squeeze()] = color_dict[i][2]\n",
    "            rgba_img[:,:,:,3][(new_labels == i).squeeze()] = 128\n",
    "    tifffile.imwrite(f\"{name}.tif\",rgba_img.to(torch.uint8).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afd7590e-fac3-4565-9f13-b6cfebfe837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tifffile.imwrite(\"step1.tif\",step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64bbac7-197a-4db4-8435-56d171c70249",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_color(step2,\"step2\")\n",
    "to_color(step3,\"step3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38125188-7119-43b9-96a7-b300eb040fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_color(step4,\"step4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "351be40a-ace9-455d-af66-3c34fc196692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step3.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bdae2cd-8390-459f-879d-6220a63f37b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label = tifffile.imread(\"../data/fixed_labels/Mix 2_003_label.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0928b22-2fd8-45e2-a20d-88edc75a7b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(out,label,name):\n",
    "    comparison = torch.zeros(label.shape)\n",
    "            \n",
    "    comparison[out == label] = 2 # Finds correctly labelled pixels\n",
    "    comparison[out != label] = 1 # Finds wrongly labelled pixels\n",
    "    comparison[out == 0] = 0    # Removes background pixels\n",
    "    \n",
    "    \n",
    "    rgba_img = torch.zeros(256,128,128,4)\n",
    "    \n",
    "    rgba_img[:,:,:,1][comparison == 2] = 128 # Set colour of correctly labelled pixels to green\n",
    "    rgba_img[:,:,:,3][comparison == 2] = 128 # Set transparency of correctly labelled pixels\n",
    "    rgba_img[:,:,:,0][comparison == 1] = 128 # Set colour of wrongly labelled pixels to red\n",
    "    rgba_img[:,:,:,3][comparison == 1] = 128 # Set transparency of wrongly labelled pixels\n",
    "    rgba_img[:,:,:,3][comparison == 0] = 255 # Set transparency of background (fully transparent)\n",
    "    tifffile.imwrite(f\"{name}.tif\",rgba_img.to(torch.uint8).numpy())\n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ddbace82-4ae2-43c2-90c4-9c8670726b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_label = compare(real_label,step4,\"pseudo_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbbd5d29-c990-4560-ad1f-2d97d94e983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = f\"{_PATH_DATA}/mixed_crop/Mix 2/Mix 2_003_centroids.csv\"\n",
    "df_label = pd.read_csv(label_path,sep=\";\")\n",
    "df_label = df_label.round().astype({\"PosX\":\"int16\",\"PosY\":\"int16\",\"PosZ\":\"int16\"})\n",
    "\n",
    "df_label[\"PosX_resized\"] = (df_label.PosX/650)*128\n",
    "df_label[\"PosY_resized\"] = (df_label.PosY/650)*128\n",
    "df_label[\"PosZ_resized\"] = (df_label.PosZ/900)*256\n",
    "df_label = df_label.round().astype({\"PosX_resized\":\"int16\",\"PosY_resized\":\"int16\",\"PosZ_resized\":\"int16\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "724a31d1-c3a5-4bcb-9ad2-dd195b2f5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = np.zeros(real_label.shape,dtype=\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14192df5-000b-4ce3-a866-cdda190e5955",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df_label.iterrows():\n",
    "    markers[item[1].PosZ_resized,item[1].PosY_resized,item[1].PosX_resized] = Label.from_abbreviation(item[1].Caption).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a090c21d-92cf-476c-aa3a-28d82f58ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_img = markers.astype(\"int16\")\n",
    "dists = np.zeros((13,256,128,128))\n",
    "label_img[label_img == 0] = -1\n",
    "for i in range(1,13):\n",
    "    label_img[label_img == i] = 0\n",
    "    dists[i,:,:,:] = ndi.distance_transform_cdt(label_img)\n",
    "    label_img[label_img == 0] = i\n",
    "\n",
    "dists[0,:,:,:] = np.inf\n",
    "dists[dists == -1] = np.inf\n",
    "dists = np.argmin(dists,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "caaf2e20-0aef-4859-85f1-647bcd5048c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pseudo_label = compare(real_label,dists,\"no_pseudo_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e5fddeb-bec0-457b-b5e5-867a4a57a11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0901397129312702"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo_label[pseudo_label == 1].shape[0]/pseudo_label[pseudo_label != 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55cf83f6-07a3-48e4-8c9b-7bfdfef77539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14002918784652332"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_pseudo_label[no_pseudo_label == 1].shape[0]/no_pseudo_label[no_pseudo_label != 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "83ea7b3f-99bf-4e20-8cd2-6760e1448037",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = pd.read_csv(f\"{_PATH_DATA}/fixed_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6c4f9536-e8d2-4fba-8e8e-bbbbce8d0152",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_paths = glob(\"metric_distance_labels/real/**/*.tif\",root_dir=_PATH_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1d91d15c-0fee-4aa5-b3db-8850a2c74d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[\"index\"] = paths.img_path.apply(lambda x: x.split(\"/\")[-1][:-4])\n",
    "paths.set_index(\"index\",inplace=True)\n",
    "paths[\"metric_path\"] = \"\"\n",
    "\n",
    "for path in metric_paths:\n",
    "    paths.loc[path.split(\"/\")[-1][:-14],\"metric_path\"] = path\n",
    "paths.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1cb3f72f-a0ad-4d66-a3bf-939e6a1057c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mix 10_013\n",
      "pseudo_label better\n",
      "Mix 2_000\n",
      "pseudo_label better\n",
      "Mix 2_001\n",
      "pseudo_label better\n",
      "Mix 2_002\n",
      "pseudo_label better\n",
      "Mix 2_003\n",
      "pseudo_label better\n",
      "Mix 2_004\n",
      "pseudo_label worse\n",
      "Mix 2_005\n",
      "pseudo_label better\n",
      "Mix 2_006\n",
      "pseudo_label better\n",
      "Mix 2_007\n",
      "pseudo_label better\n",
      "Mix 2_008\n",
      "pseudo_label worse\n",
      "Mix 2_009\n",
      "pseudo_label better\n",
      "Mix 2_010\n",
      "pseudo_label better\n",
      "Mix 2_011\n",
      "pseudo_label better\n",
      "Mix 2_012\n",
      "pseudo_label better\n",
      "Mix 2_013\n",
      "pseudo_label better\n",
      "Mix 3_000\n",
      "pseudo_label better\n",
      "Mix 3_001\n",
      "pseudo_label better\n",
      "Mix 3_002\n",
      "pseudo_label worse\n"
     ]
    }
   ],
   "source": [
    "paths[\"pseudo_label\"] = 0.0\n",
    "paths[\"no_pseudo_label\"] = 0.0\n",
    "for items in paths.iterrows():\n",
    "    img_path = items[1].img_path\n",
    "    label_path = items[1].label_path\n",
    "    metric_path = items[1].metric_path\n",
    "    if \"Mix 2\" in items[0]:\n",
    "        centroid_path = f\"{_PATH_DATA}/mixed_crop/Mix 2/{items[0]}_centroids.csv\"\n",
    "    elif \"Mix 3\" in items[0]:\n",
    "        centroid_path = f\"{_PATH_DATA}/mixed_crop/Mix 3/{items[0]}_centroids.csv\"\n",
    "    elif \"Mix 10\" in items[0]:\n",
    "        centroid_path = f\"{_PATH_DATA}/mixed_crop/Mix 10/{items[0]}_centroids.csv\"\n",
    "        \n",
    "    df_label = pd.read_csv(centroid_path,sep=\";\")\n",
    "    df_label = df_label.round().astype({\"PosX\":\"int16\",\"PosY\":\"int16\",\"PosZ\":\"int16\"})\n",
    "    \n",
    "    df_label[\"PosX_resized\"] = (df_label.PosX/650)*128\n",
    "    df_label[\"PosY_resized\"] = (df_label.PosY/650)*128\n",
    "    df_label[\"PosZ_resized\"] = (df_label.PosZ/900)*256\n",
    "    df_label = df_label.round().astype({\"PosX_resized\":\"int16\",\"PosY_resized\":\"int16\",\"PosZ_resized\":\"int16\"})\n",
    "    markers = np.zeros(real_label.shape,dtype=\"int8\")\n",
    "    for item in df_label.iterrows():\n",
    "        markers[item[1].PosZ_resized,item[1].PosY_resized,item[1].PosX_resized] = Label.from_abbreviation(item[1].Caption).value\n",
    "    label_img = markers.astype(\"int16\")\n",
    "    dists = np.zeros((13,256,128,128))\n",
    "    label_img[label_img == 0] = -1\n",
    "    for i in range(1,13):\n",
    "        label_img[label_img == i] = 0\n",
    "        dists[i,:,:,:] = ndi.distance_transform_cdt(label_img)\n",
    "        label_img[label_img == 0] = i\n",
    "    \n",
    "    dists[0,:,:,:] = np.inf\n",
    "    dists[dists == -1] = np.inf\n",
    "    dists = np.argmin(dists,axis=0)\n",
    "\n",
    "    no_pseudo_label = dists.copy()\n",
    "\n",
    "    real_label = tifffile.imread(f\"{_PATH_DATA}/{label_path}\")\n",
    "    pseudo_label = tifffile.imread(f\"{_PATH_DATA}/{metric_path}\")\n",
    "    \n",
    "    no_pseudo_label = compare(real_label,no_pseudo_label,f\"{items[0]}_no_pseudo_label\")\n",
    "    pseudo_label = compare(real_label,pseudo_label,f\"{items[0]}_pseudo_label\")\n",
    "\n",
    "    paths.loc[items[0],\"pseudo_label\"] = pseudo_label[pseudo_label == 1].shape[0]/pseudo_label[pseudo_label != 0].shape[0]\n",
    "    paths.loc[items[0],\"no_pseudo_label\"] = no_pseudo_label[no_pseudo_label == 1].shape[0]/no_pseudo_label[no_pseudo_label != 0].shape[0]\n",
    "    \n",
    "    print(items[0])\n",
    "    if paths.loc[items[0],\"pseudo_label\"] > paths.loc[items[0],\"no_pseudo_label\"]:\n",
    "        print(\"pseudo_label worse\")\n",
    "    else:\n",
    "        print(\"pseudo_label better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f3c96e8b-974f-42e9-acb3-e42851116653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11387043845445072"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths.pseudo_label.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "89053be4-b658-4beb-80d1-caac4a0be90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16018541421198731"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths.no_pseudo_label.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6ef76ce3-4e84-452f-af68-f466902f01b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[\"difference\"] = paths.pseudo_label - paths.no_pseudo_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a6916b8e-37e0-41f6-8ea1-300c425f574d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06173175411154719"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(paths[paths[\"difference\"] < 0].difference.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6251490c-1e50-44ce-8d60-dd943dd4f733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030768916012516207"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(paths[paths[\"difference\"] > 0].difference.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "464a564c-4ad2-48a7-9065-8cc3ede1cf02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>metric_path</th>\n",
       "      <th>pseudo_label</th>\n",
       "      <th>no_pseudo_label</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mix 10_013</th>\n",
       "      <td>real_256/train/00/Mix 10_013.tif</td>\n",
       "      <td>fixed_labels/Mix 10_013_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 10/Mix 10_013_...</td>\n",
       "      <td>0.063131</td>\n",
       "      <td>0.121968</td>\n",
       "      <td>-0.058837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 2_000</th>\n",
       "      <td>real_256/validation/01/Mix 2_000.tif</td>\n",
       "      <td>fixed_labels/Mix 2_000_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 2/Mix 2_000_di...</td>\n",
       "      <td>0.064960</td>\n",
       "      <td>0.083713</td>\n",
       "      <td>-0.018753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 2_001</th>\n",
       "      <td>real_256/validation/00/Mix 2_001.tif</td>\n",
       "      <td>fixed_labels/Mix 2_001_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 2/Mix 2_001_di...</td>\n",
       "      <td>0.140770</td>\n",
       "      <td>0.197243</td>\n",
       "      <td>-0.056473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 2_002</th>\n",
       "      <td>real_256/train/01/Mix 2_002.tif</td>\n",
       "      <td>fixed_labels/Mix 2_002_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 2/Mix 2_002_di...</td>\n",
       "      <td>0.227463</td>\n",
       "      <td>0.263601</td>\n",
       "      <td>-0.036138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 2_003</th>\n",
       "      <td>real_256/train/01/Mix 2_003.tif</td>\n",
       "      <td>fixed_labels/Mix 2_003_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 2/Mix 2_003_di...</td>\n",
       "      <td>0.090140</td>\n",
       "      <td>0.140029</td>\n",
       "      <td>-0.049889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 2_004</th>\n",
       "      <td>real_256/train/01/Mix 2_004.tif</td>\n",
       "      <td>fixed_labels/Mix 2_004_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 2/Mix 2_004_di...</td>\n",
       "      <td>0.254518</td>\n",
       "      <td>0.228082</td>\n",
       "      <td>0.026437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 2_005</th>\n",
       "      <td>real_256/validation/00/Mix 2_005.tif</td>\n",
       "      <td>fixed_labels/Mix 2_005_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 2/Mix 2_005_di...</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.101310</td>\n",
       "      <td>-0.101219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 2_006</th>\n",
       "      <td>real_256/train/01/Mix 2_006.tif</td>\n",
       "      <td>fixed_labels/Mix 2_006_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 2/Mix 2_006_di...</td>\n",
       "      <td>0.020473</td>\n",
       "      <td>0.043148</td>\n",
       "      <td>-0.022674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 2_007</th>\n",
       "      <td>real_256/train/01/Mix 2_007.tif</td>\n",
       "      <td>fixed_labels/Mix 2_007_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 2/Mix 2_007_di...</td>\n",
       "      <td>0.005620</td>\n",
       "      <td>0.086788</td>\n",
       "      <td>-0.081168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 2_008</th>\n",
       "      <td>real_256/train/01/Mix 2_008.tif</td>\n",
       "      <td>fixed_labels/Mix 2_008_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 2/Mix 2_008_di...</td>\n",
       "      <td>0.341747</td>\n",
       "      <td>0.315580</td>\n",
       "      <td>0.026167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 2_009</th>\n",
       "      <td>real_256/train/01/Mix 2_009.tif</td>\n",
       "      <td>fixed_labels/Mix 2_009_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 2/Mix 2_009_di...</td>\n",
       "      <td>0.053190</td>\n",
       "      <td>0.108450</td>\n",
       "      <td>-0.055260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 2_010</th>\n",
       "      <td>real_256/train/01/Mix 2_010.tif</td>\n",
       "      <td>fixed_labels/Mix 2_010_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 2/Mix 2_010_di...</td>\n",
       "      <td>0.160921</td>\n",
       "      <td>0.274673</td>\n",
       "      <td>-0.113752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 2_011</th>\n",
       "      <td>real_256/train/01/Mix 2_011.tif</td>\n",
       "      <td>fixed_labels/Mix 2_011_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 2/Mix 2_011_di...</td>\n",
       "      <td>0.185066</td>\n",
       "      <td>0.305068</td>\n",
       "      <td>-0.120002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 2_012</th>\n",
       "      <td>real_256/train/01/Mix 2_012.tif</td>\n",
       "      <td>fixed_labels/Mix 2_012_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 2/Mix 2_012_di...</td>\n",
       "      <td>0.111526</td>\n",
       "      <td>0.246319</td>\n",
       "      <td>-0.134793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 2_013</th>\n",
       "      <td>real_256/validation/00/Mix 2_013.tif</td>\n",
       "      <td>fixed_labels/Mix 2_013_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 2/Mix 2_013_di...</td>\n",
       "      <td>0.010769</td>\n",
       "      <td>0.028531</td>\n",
       "      <td>-0.017762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 3_000</th>\n",
       "      <td>real_256/train/02/Mix 3_000.tif</td>\n",
       "      <td>fixed_labels/Mix 3_000_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 3/Mix 3_000_di...</td>\n",
       "      <td>0.086163</td>\n",
       "      <td>0.109879</td>\n",
       "      <td>-0.023716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 3_001</th>\n",
       "      <td>real_256/train/02/Mix 3_001.tif</td>\n",
       "      <td>fixed_labels/Mix 3_001_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 3/Mix 3_001_di...</td>\n",
       "      <td>0.142999</td>\n",
       "      <td>0.178539</td>\n",
       "      <td>-0.035540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 3_002</th>\n",
       "      <td>real_256/train/02/Mix 3_002.tif</td>\n",
       "      <td>fixed_labels/Mix 3_002_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 3/Mix 3_002_di...</td>\n",
       "      <td>0.090121</td>\n",
       "      <td>0.050418</td>\n",
       "      <td>0.039703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        img_path  \\\n",
       "index                                              \n",
       "Mix 10_013      real_256/train/00/Mix 10_013.tif   \n",
       "Mix 2_000   real_256/validation/01/Mix 2_000.tif   \n",
       "Mix 2_001   real_256/validation/00/Mix 2_001.tif   \n",
       "Mix 2_002        real_256/train/01/Mix 2_002.tif   \n",
       "Mix 2_003        real_256/train/01/Mix 2_003.tif   \n",
       "Mix 2_004        real_256/train/01/Mix 2_004.tif   \n",
       "Mix 2_005   real_256/validation/00/Mix 2_005.tif   \n",
       "Mix 2_006        real_256/train/01/Mix 2_006.tif   \n",
       "Mix 2_007        real_256/train/01/Mix 2_007.tif   \n",
       "Mix 2_008        real_256/train/01/Mix 2_008.tif   \n",
       "Mix 2_009        real_256/train/01/Mix 2_009.tif   \n",
       "Mix 2_010        real_256/train/01/Mix 2_010.tif   \n",
       "Mix 2_011        real_256/train/01/Mix 2_011.tif   \n",
       "Mix 2_012        real_256/train/01/Mix 2_012.tif   \n",
       "Mix 2_013   real_256/validation/00/Mix 2_013.tif   \n",
       "Mix 3_000        real_256/train/02/Mix 3_000.tif   \n",
       "Mix 3_001        real_256/train/02/Mix 3_001.tif   \n",
       "Mix 3_002        real_256/train/02/Mix 3_002.tif   \n",
       "\n",
       "                                   label_path  \\\n",
       "index                                           \n",
       "Mix 10_013  fixed_labels/Mix 10_013_label.tif   \n",
       "Mix 2_000    fixed_labels/Mix 2_000_label.tif   \n",
       "Mix 2_001    fixed_labels/Mix 2_001_label.tif   \n",
       "Mix 2_002    fixed_labels/Mix 2_002_label.tif   \n",
       "Mix 2_003    fixed_labels/Mix 2_003_label.tif   \n",
       "Mix 2_004    fixed_labels/Mix 2_004_label.tif   \n",
       "Mix 2_005    fixed_labels/Mix 2_005_label.tif   \n",
       "Mix 2_006    fixed_labels/Mix 2_006_label.tif   \n",
       "Mix 2_007    fixed_labels/Mix 2_007_label.tif   \n",
       "Mix 2_008    fixed_labels/Mix 2_008_label.tif   \n",
       "Mix 2_009    fixed_labels/Mix 2_009_label.tif   \n",
       "Mix 2_010    fixed_labels/Mix 2_010_label.tif   \n",
       "Mix 2_011    fixed_labels/Mix 2_011_label.tif   \n",
       "Mix 2_012    fixed_labels/Mix 2_012_label.tif   \n",
       "Mix 2_013    fixed_labels/Mix 2_013_label.tif   \n",
       "Mix 3_000    fixed_labels/Mix 3_000_label.tif   \n",
       "Mix 3_001    fixed_labels/Mix 3_001_label.tif   \n",
       "Mix 3_002    fixed_labels/Mix 3_002_label.tif   \n",
       "\n",
       "                                                  metric_path  pseudo_label  \\\n",
       "index                                                                         \n",
       "Mix 10_013  metric_distance_labels/real/Mix 10/Mix 10_013_...      0.063131   \n",
       "Mix 2_000   metric_distance_labels/real/Mix 2/Mix 2_000_di...      0.064960   \n",
       "Mix 2_001   metric_distance_labels/real/Mix 2/Mix 2_001_di...      0.140770   \n",
       "Mix 2_002   metric_distance_labels/real/Mix 2/Mix 2_002_di...      0.227463   \n",
       "Mix 2_003   metric_distance_labels/real/Mix 2/Mix 2_003_di...      0.090140   \n",
       "Mix 2_004   metric_distance_labels/real/Mix 2/Mix 2_004_di...      0.254518   \n",
       "Mix 2_005   metric_distance_labels/real/Mix 2/Mix 2_005_di...      0.000091   \n",
       "Mix 2_006   metric_distance_labels/real/Mix 2/Mix 2_006_di...      0.020473   \n",
       "Mix 2_007   metric_distance_labels/real/Mix 2/Mix 2_007_di...      0.005620   \n",
       "Mix 2_008   metric_distance_labels/real/Mix 2/Mix 2_008_di...      0.341747   \n",
       "Mix 2_009   metric_distance_labels/real/Mix 2/Mix 2_009_di...      0.053190   \n",
       "Mix 2_010   metric_distance_labels/real/Mix 2/Mix 2_010_di...      0.160921   \n",
       "Mix 2_011   metric_distance_labels/real/Mix 2/Mix 2_011_di...      0.185066   \n",
       "Mix 2_012   metric_distance_labels/real/Mix 2/Mix 2_012_di...      0.111526   \n",
       "Mix 2_013   metric_distance_labels/real/Mix 2/Mix 2_013_di...      0.010769   \n",
       "Mix 3_000   metric_distance_labels/real/Mix 3/Mix 3_000_di...      0.086163   \n",
       "Mix 3_001   metric_distance_labels/real/Mix 3/Mix 3_001_di...      0.142999   \n",
       "Mix 3_002   metric_distance_labels/real/Mix 3/Mix 3_002_di...      0.090121   \n",
       "\n",
       "            no_pseudo_label  difference  \n",
       "index                                    \n",
       "Mix 10_013         0.121968   -0.058837  \n",
       "Mix 2_000          0.083713   -0.018753  \n",
       "Mix 2_001          0.197243   -0.056473  \n",
       "Mix 2_002          0.263601   -0.036138  \n",
       "Mix 2_003          0.140029   -0.049889  \n",
       "Mix 2_004          0.228082    0.026437  \n",
       "Mix 2_005          0.101310   -0.101219  \n",
       "Mix 2_006          0.043148   -0.022674  \n",
       "Mix 2_007          0.086788   -0.081168  \n",
       "Mix 2_008          0.315580    0.026167  \n",
       "Mix 2_009          0.108450   -0.055260  \n",
       "Mix 2_010          0.274673   -0.113752  \n",
       "Mix 2_011          0.305068   -0.120002  \n",
       "Mix 2_012          0.246319   -0.134793  \n",
       "Mix 2_013          0.028531   -0.017762  \n",
       "Mix 3_000          0.109879   -0.023716  \n",
       "Mix 3_001          0.178539   -0.035540  \n",
       "Mix 3_002          0.050418    0.039703  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "70bf46f7-fe1e-4d91-a966-1f0ff47bdd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>metric_path</th>\n",
       "      <th>pseudo_label</th>\n",
       "      <th>no_pseudo_label</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mix 2_004</th>\n",
       "      <td>real_256/train/01/Mix 2_004.tif</td>\n",
       "      <td>fixed_labels/Mix 2_004_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 2/Mix 2_004_di...</td>\n",
       "      <td>0.254518</td>\n",
       "      <td>0.228082</td>\n",
       "      <td>0.026437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 2_008</th>\n",
       "      <td>real_256/train/01/Mix 2_008.tif</td>\n",
       "      <td>fixed_labels/Mix 2_008_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 2/Mix 2_008_di...</td>\n",
       "      <td>0.341747</td>\n",
       "      <td>0.315580</td>\n",
       "      <td>0.026167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mix 3_002</th>\n",
       "      <td>real_256/train/02/Mix 3_002.tif</td>\n",
       "      <td>fixed_labels/Mix 3_002_label.tif</td>\n",
       "      <td>metric_distance_labels/real/Mix 3/Mix 3_002_di...</td>\n",
       "      <td>0.090121</td>\n",
       "      <td>0.050418</td>\n",
       "      <td>0.039703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  img_path                        label_path  \\\n",
       "index                                                                          \n",
       "Mix 2_004  real_256/train/01/Mix 2_004.tif  fixed_labels/Mix 2_004_label.tif   \n",
       "Mix 2_008  real_256/train/01/Mix 2_008.tif  fixed_labels/Mix 2_008_label.tif   \n",
       "Mix 3_002  real_256/train/02/Mix 3_002.tif  fixed_labels/Mix 3_002_label.tif   \n",
       "\n",
       "                                                 metric_path  pseudo_label  \\\n",
       "index                                                                        \n",
       "Mix 2_004  metric_distance_labels/real/Mix 2/Mix 2_004_di...      0.254518   \n",
       "Mix 2_008  metric_distance_labels/real/Mix 2/Mix 2_008_di...      0.341747   \n",
       "Mix 3_002  metric_distance_labels/real/Mix 3/Mix 3_002_di...      0.090121   \n",
       "\n",
       "           no_pseudo_label  difference  \n",
       "index                                   \n",
       "Mix 2_004         0.228082    0.026437  \n",
       "Mix 2_008         0.315580    0.026167  \n",
       "Mix 3_002         0.050418    0.039703  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths[paths[\"difference\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef35f61-809c-4367-b90e-2634fa91d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import _PATH_DATA, _PATH_MODELS, _PROJECT_ROOT\n",
    "from src.data.dataloaders import MetricDataset#, BugNISTDataModule\n",
    "from src.models.unet import UNet_pl\n",
    "from src.models.swin_unetr import SwinUNETR_pl as SwinUNETR\n",
    "import torch\n",
    "import torch._dynamo\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import tifffile\n",
    "from skimage import measure\n",
    "import numpy as np\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "951322f6-0702-4fac-b006-c87d2d48c93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.data.dataloaders import Label\n",
    "# temp = Label.abbreviation_dict()\n",
    "# [temp[i] for i in range(1,len(temp)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "467a7622-7568-4a80-a581-1d8f11e12107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, precision, recall, dice_score):\n",
    "    \n",
    "    color_dict = {1:[255,0,0],2:[0,255,0],3:[0,0,255],4:[255,255,0],5:[255,0,255],6:[0,255,255],7:[161,161,255],8:[171,128,84],9:[255,128,191],10:[135,89,179],11:[255,191,128],12:[0,85,0]}\n",
    "    accuracies = []\n",
    "    for k,(img,label) in enumerate(tqdm(test_loader, unit=\"batch\")):\n",
    "        img = img.to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            preds = model(img);\n",
    "            preds_sm = preds.softmax(dim=1).cpu().numpy();\n",
    "    \n",
    "            class_props = []\n",
    "            for i in range(1, 13):  # For each class channel\n",
    "                props = measure.regionprops(\n",
    "            \n",
    "                    #                        v-- softmax channel for this class\n",
    "            \n",
    "                    measure.label(preds_sm[0, i] > 0.25)  # Threshold softmax probability at 0.25\n",
    "            \n",
    "                )\n",
    "            \n",
    "                props = [p for p in props if p.area > 5**3]  # Remove small connected components\n",
    "            \n",
    "                class_props.append(props)\n",
    "            \n",
    "            for i, props in enumerate(class_props):\n",
    "                # For every found connected component\n",
    "                for p in props:\n",
    "                    bb = p.bbox\n",
    "            \n",
    "                    # Sets the found connected component to the mean value of the connected component\n",
    "                    preds_sm[0,i+1,bb[0]:bb[3],bb[1]:bb[4],bb[2]:bb[5]][p.image] = preds_sm[0,i+1,bb[0]:bb[3],bb[1]:bb[4],bb[2]:bb[5]][p.image].mean()\n",
    "            \n",
    "            out = torch.Tensor(preds_sm).softmax(dim=1).argmax(dim=1).to(torch.uint8)\n",
    "            \n",
    "            precision.update(out,label)\n",
    "            recall.update(out,label)\n",
    "            dice_score.update(out,label)\n",
    "            accuracies.append(sum(out[out != 0] == label[out != 0])/len(label[out != 0]))\n",
    "\n",
    "    accuracies = torch.Tensor(accuracies)\n",
    "    print(\"simple precision\",accuracies.mean())\n",
    "    print(\"precision\",precision.compute())\n",
    "    print(\"recall\",recall.compute())\n",
    "    print(\"dice\",dice_score.compute())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ccc3c87-a18b-40df-b1c9-3d1faa2ab61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_params(model_type,size,version):\n",
    "    precision = torchmetrics.Precision(task=\"multiclass\", num_classes=13, average=\"weighted\", ignore_index=0)\n",
    "    recall = torchmetrics.Recall(task=\"multiclass\", num_classes=13, average=\"weighted\", ignore_index=0)\n",
    "    dice_score = torchmetrics.Dice(num_classes=13, ignore_index=0)\n",
    "    \n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "    \n",
    "    test_loader = DataLoader(MetricDataset(fixed=True),\n",
    "                         batch_size=1,\n",
    "                         num_workers=0)\n",
    "    \n",
    "    if model_type == \"small\":\n",
    "        model = UNet_pl(\n",
    "            spatial_dims=3,\n",
    "            in_channels=1,\n",
    "            out_channels=13,\n",
    "            channels=(4, 8, 16, 32, 64),\n",
    "            strides=(2, 2, 2, 2),\n",
    "            lr=1,\n",
    "        )\n",
    "    elif model_type == \"large\":\n",
    "        model = UNet_pl(\n",
    "                spatial_dims=3,\n",
    "                in_channels=1,\n",
    "                out_channels=13,\n",
    "                channels=(16, 32, 64, 128, 256, 512),\n",
    "                strides=(2, 2, 2, 2, 2),\n",
    "                num_res_units = 3,\n",
    "            )\n",
    "    elif model_type == \"swin\":\n",
    "        model = SwinUNETR(img_size=(256,128,128), in_channels=1, out_channels=13, feature_size=24)\n",
    "    \n",
    "    \n",
    "    if version == \"single\":\n",
    "        model_path = glob(f\"{_PATH_MODELS}/{model_type}_{version}*/*.ckpt\")[0]\n",
    "    else:\n",
    "        model_path = glob(f\"{_PATH_MODELS}/{model_type}{size}{version}*/*.ckpt\")[0]\n",
    "    model.load_state_dict(torch.load(model_path, map_location=None)['state_dict'], strict=True)\n",
    "        \n",
    "    torch._dynamo.config.suppress_errors = True\n",
    "    if model_type != \"swin\":\n",
    "        model = torch.compile(model)\n",
    "    model.eval();\n",
    "    model = model.to(\"cuda\")\n",
    "    \n",
    "    test(model,test_loader, precision, recall, dice_score,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b57a5a3e-4563-4ef0-8184-c98295537c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 18/18 [01:54<00:00,  6.34s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple precision tensor(0.5320)\n",
      "precision tensor(0.7116)\n",
      "recall tensor(0.5755)\n",
      "dice tensor(0.5673)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_type = \"small\"\n",
    "size = 50000\n",
    "version=\"v3\"\n",
    "test_params(model_type,size,version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcfbc72d-d4e8-4a50-b01f-83f1632b9860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 18/18 [02:03<00:00,  6.88s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple accuracy tensor(0.7770)\n",
      "accuracy tensor(0.7584)\n",
      "precision tensor(0.8778)\n",
      "recall tensor(0.7584)\n",
      "dice tensor(0.7759)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_type = \"large\"\n",
    "size = 50000\n",
    "version=\"v3\"\n",
    "test_params(model_type,size,version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fba8e73-1290-4fd7-91eb-f974656f802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 18/18 [02:02<00:00,  6.82s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple accuracy tensor(0.7759)\n",
      "accuracy tensor(0.7931)\n",
      "precision tensor(0.8696)\n",
      "recall tensor(0.7931)\n",
      "dice tensor(0.7927)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_type = \"swin\"\n",
    "size = 20000\n",
    "version=\"v3\"\n",
    "test_params(model_type,size,version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55ea60df-fc65-4271-99f9-87c08c65580c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 18/18 [01:32<00:00,  5.12s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple accuracy tensor(0.0687)\n",
      "accuracy tensor(0.0182)\n",
      "precision tensor(0.2516)\n",
      "recall tensor(0.0182)\n",
      "dice tensor(0.0295)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_type = \"small\"\n",
    "size = 50000\n",
    "version=\"single\"\n",
    "test_params(model_type,size,version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68b8fd6d-0173-4ef2-8692-f0db5bec9e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 18/18 [02:15<00:00,  7.50s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple accuracy tensor(0.0995)\n",
      "accuracy tensor(0.1224)\n",
      "precision tensor(0.3058)\n",
      "recall tensor(0.1224)\n",
      "dice tensor(0.1229)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_type = \"large\"\n",
    "size = 50000\n",
    "version=\"single\"\n",
    "test_params(model_type,size,version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8005ca2d-4fe8-4cae-b17e-4ff3367a7cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 18/18 [01:44<00:00,  5.79s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple accuracy tensor(0.1372)\n",
      "accuracy tensor(0.0599)\n",
      "precision tensor(0.2731)\n",
      "recall tensor(0.0599)\n",
      "dice tensor(0.0829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_type = \"swin\"\n",
    "size = 20000\n",
    "version=\"single\"\n",
    "test_params(model_type,size,version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd5da7-e6ef-473a-845a-9f21f1f893e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
